{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(C) 2017 by [Damir Cavar](http://cavar.me/damir/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version:** 1.0, March 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/) ([CA BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial related to the discussion of Decision Tree classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial was developed as part of my course material for the course Machine Learning for Computational Linguistics in the [Computational Linguistics Program](http://cl.indiana.edu/) of the [Department of Linguistics](http://www.indiana.edu/~lingdept/) at [Indiana University](https://www.indiana.edu/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For discussion and explanation of the algorithms and use of implementations in Python libraries, see the following documentation online:\n",
    "\n",
    "- [Scikit Learn Decision Trees](http://scikit-learn.org/stable/modules/tree.html)\n",
    "- Christopher Roach's [Building Decision Trees in Python](http://www.onlamp.com/pub/a/python/2006/02/09/ai_decision_trees.html?page=1) (see his [Github repository](https://github.com/croach/dtree))\n",
    "- Jason Brownlee's [How To Implement The Decision Tree Algorithm From Scratch In Python](http://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reimplementation of Roach's Building Decision Trees in Python 3.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example code is taken from Christopher Roach's [Building Decision Trees in Python](http://www.onlamp.com/pub/a/python/2006/02/09/ai_decision_trees.html?page=1). It has been slightly simplified. You can download the code from his [Github repository](https://github.com/croach/dtree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections, math, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roach: \"This module holds functions that are responsible for creating a new decision tree and for using the tree for data classificiation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def majority_value(data, target_attr):\n",
    "    \"\"\"\n",
    "    Creates a list of all values in the target attribute for each record\n",
    "    in the data list object, and returns the value that appears in this list\n",
    "    the most frequently.\n",
    "    \"\"\"\n",
    "    return most_frequent([record[target_attr] for record in data])\n",
    "\n",
    "\n",
    "def most_frequent(lst):\n",
    "    \"Returns the item that appears most frequently in the given list.\"\n",
    "\n",
    "    highest_freq = 0\n",
    "    most_freq = None\n",
    "\n",
    "    for val in unique(lst):\n",
    "        if lst.count(val) > highest_freq:\n",
    "            most_freq = val\n",
    "            highest_freq = lst.count(val)\n",
    "            \n",
    "    return most_freq\n",
    "\n",
    "\n",
    "def unique(lst):\n",
    "    return list(set(lst))\n",
    "\n",
    "\n",
    "def get_values(data, attr):\n",
    "    \"\"\"\n",
    "    Creates a list of values in the chosen attribut for each record in data,\n",
    "    prunes out all of the redundant values, and return the list.  \n",
    "    \"\"\"\n",
    "    return unique([record[attr] for record in data])\n",
    "\n",
    "\n",
    "def choose_attribute(data, attributes, target_attr, fitness):\n",
    "    \"\"\"\n",
    "    Cycles through all the attributes and returns the attribute with the\n",
    "    highest information gain (or lowest entropy).\n",
    "    \"\"\"\n",
    "    best_gain = 0.0\n",
    "    best_attr = None\n",
    "\n",
    "    for attr in attributes:\n",
    "        gain = fitness(data, attr, target_attr)\n",
    "        if (gain >= best_gain and attr != target_attr):\n",
    "            best_gain = gain\n",
    "            best_attr = attr\n",
    "                \n",
    "    return best_attr\n",
    "\n",
    "\n",
    "def get_examples(data, attr, value):\n",
    "    \"\"\"\n",
    "    Returns a list of all the records in <data> with the value of <attr>\n",
    "    matching the given value.\n",
    "    \"\"\"\n",
    "    rtn_lst = []\n",
    "    \n",
    "    if not data:\n",
    "        return rtn_lst\n",
    "    else:\n",
    "        record = data.pop()\n",
    "        if record[attr] == value:\n",
    "            rtn_lst.append(record)\n",
    "            rtn_lst.extend(get_examples(data, attr, value))\n",
    "            return rtn_lst\n",
    "        else:\n",
    "            rtn_lst.extend(get_examples(data, attr, value))\n",
    "            return rtn_lst\n",
    "\n",
    "def get_classification(record, tree):\n",
    "    \"\"\"\n",
    "    This function recursively traverses the decision tree and returns a\n",
    "    classification for the given record.\n",
    "    \"\"\"\n",
    "    # If the current node is a string, then we've reached a leaf node and\n",
    "    # we can return it as our answer\n",
    "    if type(tree) == type(\"string\"):\n",
    "        return tree\n",
    "    if tree == None:\n",
    "        return tree\n",
    "\n",
    "    # Traverse the tree further until a leaf node is found.\n",
    "    myKeys = list(tree.keys())\n",
    "    attr = myKeys[0]\n",
    "    t = tree[attr][record[attr]]\n",
    "    return get_classification(record, t)\n",
    "\n",
    "\n",
    "def classify(tree, data):\n",
    "    \"\"\"\n",
    "    Returns a list of classifications for each of the records in the data\n",
    "    list as determined by the given decision tree.\n",
    "    \"\"\"\n",
    "    return [ get_classification(record, tree) for record in data ]\n",
    "\n",
    "\n",
    "def create_decision_tree(data, attributes, target_attr, fitness_func):\n",
    "    \"Returns a new decision tree based on the examples given.\"\n",
    "    vals = [record[target_attr] for record in data]\n",
    "    default = majority_value(data, target_attr)\n",
    "\n",
    "    # If the dataset is empty or the attributes list is empty, return the\n",
    "    # default value. When checking the attributes list for emptiness, we\n",
    "    # need to subtract 1 to account for the target attribute.\n",
    "    if not data or len(attributes) == 0:\n",
    "        return default\n",
    "    # If all the records in the dataset have the same classification,\n",
    "    # return that classification.\n",
    "    elif vals.count(vals[0]) == len(vals):\n",
    "        return vals[0]\n",
    "    else:\n",
    "        # Choose the next best attribute to best classify our data\n",
    "        best = choose_attribute(data, attributes, target_attr, fitness_func)\n",
    "\n",
    "        # Create a new decision tree/node with the best attribute and an empty\n",
    "        # dictionary object--we'll fill that up next.\n",
    "        # We use the collections.defaultdict function to add a function to the\n",
    "        # new tree that will be called whenever we query the tree with an\n",
    "        # attribute that does not exist.  This way we return the default value\n",
    "        # for the target attribute whenever, we have an attribute combination\n",
    "        # that wasn't seen during training.\n",
    "        tree = {best:collections.defaultdict(lambda: default)}\n",
    "\n",
    "        # Create a new decision tree/sub-node for each of the values in the\n",
    "        # best attribute field\n",
    "        for val in get_values(data, best):\n",
    "            # Create a subtree for the current value under the \"best\" field\n",
    "            subtree = create_decision_tree(\n",
    "                get_examples(data, best, val),\n",
    "                [attr for attr in attributes if attr != best],\n",
    "                target_attr,\n",
    "                fitness_func)\n",
    "\n",
    "            # Add the new subtree to the empty dictionary object in our new\n",
    "            # tree/node we just created.\n",
    "            tree[best][val] = subtree\n",
    "\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roach: \"This module contains the functions for calculating the information gain of a dataset as defined by the ID3 (Information Theoretic) heuristic.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function calculates the entropy for a given data set and the specific target attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(data, target_attr):\n",
    "    val_freq = {}\n",
    "    data_entropy = 0.0\n",
    "\n",
    "    # Compute the frequency of each of the values in the target attr\n",
    "    for record in data:\n",
    "        val_freq[record[target_attr]] = val_freq.get(record[target_attr], 0.0) + 1.0\n",
    "\n",
    "    # Compute the entropy of the data for the target attribute\n",
    "    for freq in val_freq.values():\n",
    "        data_entropy += (-freq/len(data)) * math.log(freq/len(data), 2) \n",
    "        \n",
    "    return data_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function computes the information gain (reduction in entropy) that would result by splitting the data on the chosen attribute (attr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gain(data, attr, target_attr):\n",
    "    val_freq = {}\n",
    "    subset_entropy = 0.0\n",
    "\n",
    "    # Compute the frequency of each of the values in the target attribute\n",
    "    for record in data:\n",
    "        val_freq[record[attr]] = val_freq.get(record[attr], 0.0) + 1.0\n",
    "\n",
    "    # Compute the sum of the entropy for each subset of records weighted\n",
    "    # by their probability of occuring in the training set.\n",
    "    total = sum(val_freq.values())\n",
    "    for val in val_freq.keys():\n",
    "        val_prob = val_freq[val] / total\n",
    "        data_subset = [record for record in data if record[attr] == val]\n",
    "        subset_entropy += val_prob * entropy(data_subset, target_attr)\n",
    "\n",
    "    # Subtract the entropy of the chosen attribute from the entropy of the\n",
    "    # whole data set with respect to the target attribute (and return it)\n",
    "    return (entropy(data, target_attr) - subset_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Education': 'masters', 'Marital Status': 'single', 'Age': '36 - 55', 'Income': 'high', 'Purchase?': 'will buy'}\n",
      "{'Education': 'high school', 'Marital Status': 'single', 'Age': '18 - 35', 'Income': 'low', 'Purchase?': \"won't buy\"}\n",
      "{'Education': 'masters', 'Marital Status': 'single', 'Age': '36 - 55', 'Income': 'low', 'Purchase?': 'will buy'}\n",
      "{'Education': 'bachelors', 'Marital Status': 'single', 'Age': '18 - 35', 'Income': 'high', 'Purchase?': \"won't buy\"}\n",
      "{'Education': 'high school', 'Marital Status': 'single', 'Age': '< 18', 'Income': 'low', 'Purchase?': 'will buy'}\n",
      "{'Education': 'bachelors', 'Marital Status': 'married', 'Age': '18 - 35', 'Income': 'high', 'Purchase?': \"won't buy\"}\n",
      "{'Education': 'bachelors', 'Marital Status': 'married', 'Age': '36 - 55', 'Income': 'low', 'Purchase?': \"won't buy\"}\n",
      "{'Education': 'bachelors', 'Marital Status': 'single', 'Age': '> 55', 'Income': 'high', 'Purchase?': 'will buy'}\n",
      "{'Education': 'masters', 'Marital Status': 'married', 'Age': '36 - 55', 'Income': 'low', 'Purchase?': \"won't buy\"}\n",
      "{'Education': 'masters', 'Marital Status': 'married', 'Age': '> 55', 'Income': 'low', 'Purchase?': 'will buy'}\n",
      "{'Education': 'masters', 'Marital Status': 'single', 'Age': '36 - 55', 'Income': 'high', 'Purchase?': 'will buy'}\n",
      "{'Education': 'masters', 'Marital Status': 'single', 'Age': '> 55', 'Income': 'high', 'Purchase?': 'will buy'}\n",
      "{'Education': 'high school', 'Marital Status': 'single', 'Age': '< 18', 'Income': 'high', 'Purchase?': \"won't buy\"}\n",
      "{'Education': 'masters', 'Marital Status': 'single', 'Age': '36 - 55', 'Income': 'low', 'Purchase?': 'will buy'}\n",
      "{'Education': 'high school', 'Marital Status': 'single', 'Age': '36 - 55', 'Income': 'low', 'Purchase?': 'will buy'}\n",
      "{'Education': 'high school', 'Marital Status': 'married', 'Age': '< 18', 'Income': 'low', 'Purchase?': 'will buy'}\n",
      "{'Education': 'bachelors', 'Marital Status': 'married', 'Age': '18 - 35', 'Income': 'high', 'Purchase?': \"won't buy\"}\n",
      "{'Education': 'high school', 'Marital Status': 'married', 'Age': '> 55', 'Income': 'high', 'Purchase?': 'will buy'}\n",
      "{'Education': 'bachelors', 'Marital Status': 'single', 'Age': '> 55', 'Income': 'low', 'Purchase?': 'will buy'}\n",
      "{'Education': 'high school', 'Marital Status': 'married', 'Age': '36 - 55', 'Income': 'high', 'Purchase?': \"won't buy\"}\n",
      "Purchase?\n",
      "------------------------\n",
      "\n",
      "--   Classification   --\n",
      "\n",
      "------------------------\n",
      "\n",
      "\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "won't buy\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "--   Decision Tree    --\n",
      "\n",
      "------------------------\n",
      "\n",
      "\n",
      "\n",
      "{'Age': defaultdict(<function create_decision_tree.<locals>.<lambda> at 0x106b58d08>, {'< 18': {'Income': defaultdict(<function create_decision_tree.<locals>.<lambda> at 0x106b58840>, {'high': \"won't buy\", 'low': None})}, '18 - 35': None, '> 55': None, '36 - 55': None})}\n",
      " Age\n",
      " \t < 18\n",
      "{'Income': defaultdict(<function create_decision_tree.<locals>.<lambda> at 0x106b58840>, {'high': \"won't buy\", 'low': None})}\n",
      "\t Income\n",
      "\t \t high\n",
      "won't buy\n",
      "\t\t \t->\t won't buy\n",
      "\t \t low\n",
      "None\n",
      "\t\t \t->\t None\n",
      " \t 18 - 35\n",
      "None\n",
      "\t \t->\t None\n",
      " \t > 55\n",
      "None\n",
      "\t \t->\t None\n",
      " \t 36 - 55\n",
      "None\n",
      "\t \t->\t None\n"
     ]
    }
   ],
   "source": [
    "def print_tree(tree, myStr):\n",
    "    \"\"\"\n",
    "    This function recursively crawls through the d-tree and prints it out in a\n",
    "    more readable format than a straight print of the Python dict object.  \n",
    "    \"\"\"\n",
    "    print(tree)\n",
    "    if type(tree) == dict:\n",
    "        myKeys = list(tree.keys())\n",
    "        print(myStr, myKeys[0])\n",
    "        myVals = list(tree.values())\n",
    "        for item in myVals[0].keys():\n",
    "            print(myStr, '\\t', item)\n",
    "            print_tree(myVals[0][item], myStr + \"\\t\")\n",
    "    else:\n",
    "        print(myStr, \"\\t->\\t\", tree)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Get the training and test data filenames from the user\n",
    "    training_filename = \"PyDecisionTreesData.txt\"\n",
    "    test_filename = \"PyDecisionTreesData.txt\"\n",
    "\n",
    "    training_data = []\n",
    "    try:\n",
    "        ifp = open(training_filename, mode='r', encoding='utf8')\n",
    "        attributes = [attr.strip() for attr in ifp.readline().strip().split(\",\")]\n",
    "        for line in ifp.readlines():\n",
    "            training_data.append(dict(zip(attributes,\n",
    "                             [datum.strip() for datum in line.strip().split(\",\")])))\n",
    "            print(dict(zip(attributes, [datum.strip() for datum in line.strip().split(\",\")])))\n",
    "        ifp.close()\n",
    "    except IOError:\n",
    "        print(\"Error reading from file\", training_filename)\n",
    "    # Extract the attribute names and the target attribute from the training\n",
    "    # data file.\n",
    "    \n",
    "    target_attr = attributes[-1]\n",
    "\n",
    "    # Get the training and test data from the given files\n",
    "    test_data = training_data[:]\n",
    "    print(target_attr)\n",
    "    \n",
    "    # Create the decision tree\n",
    "    dtree = create_decision_tree(training_data, attributes, target_attr, gain)\n",
    "\n",
    "    # Classify the records in the test data\n",
    "    classification = classify(dtree, test_data)\n",
    "\n",
    "    # Print the results of the test\n",
    "    print(\"------------------------\\n\")\n",
    "    print(\"--   Classification   --\\n\")\n",
    "    print(\"------------------------\\n\")\n",
    "    print(\"\\n\")\n",
    "    for item in classification: print(item)\n",
    "    \n",
    "    # Print the contents of the decision tree\n",
    "    print(\"\\n\")\n",
    "    print(\"------------------------\\n\")\n",
    "    print(\"--   Decision Tree    --\\n\")\n",
    "    print(\"------------------------\\n\")\n",
    "    print(\"\\n\")\n",
    "    print_tree(dtree, \"\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
