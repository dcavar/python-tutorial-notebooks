{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34017d26",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "&copy; 2025-2026 by [Damir Cavar](http://damir.cavar.me/)\n",
    "\n",
    "The following code examples show how tokenization works using the various different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e7fc77",
   "metadata": {},
   "source": [
    "## Word Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa86cf3",
   "metadata": {},
   "source": [
    "### NLTK Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d30e2fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c789701",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Our friend Grungle Svonitovich is going to Paris. The students are going to New York.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6ec4b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our', 'friend', 'Grungle', 'Svonitovich', 'is', 'going', 'to', 'Paris', '.', 'The', 'students', 'are', 'going', 'to', 'New', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577c860b",
   "metadata": {},
   "source": [
    "NLTK offers also a RegexpTokenizer, using regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c5bc252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30aa290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d5741b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our', 'friend', 'Grungle', 'Svonitovich', 'is', 'going', 'to', 'Paris', '.', 'The', 'students', 'are', 'going', 'to', 'New', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea9915",
   "metadata": {},
   "source": [
    "The NLTK WhiteSpaceTokenizer splits text by whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2848dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e9a3941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c4ee3349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our', 'friend', 'Grungle', 'Svonitovich', 'is', 'going', 'to', 'Paris.', 'The', 'students', 'are', 'going', 'to', 'New', 'York.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43adf0d",
   "metadata": {},
   "source": [
    "Another NLTK tokenizer splits text based on blank lines (e.g., text into paragraphs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31dcedc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b27adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Our friend Grungle Svonitovich is going to Paris.\n",
    "\n",
    "The students are going to New York.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39afe757",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BlanklineTokenizer()\n",
    "tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "866a09bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our friend Grungle Svonitovich is going to Paris.', 'The students are going to New York.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856f15be",
   "metadata": {},
   "source": [
    "## WordPiece Tokenizer\n",
    "\n",
    "The WordPiece tokenizer is used in BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a74d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install word-piece-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d30e0598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_piece_tokenizer import WordPieceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796b6f7f",
   "metadata": {},
   "source": [
    "Create a tokenizer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6594255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPieceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0d951b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Our friend Grungle Svonitovich is going to Paris. The students are going to New York.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bebee65",
   "metadata": {},
   "source": [
    "Tokenizing a sentence will return token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "186df487",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7cedfd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2256, 2767, 24665, 5575, 2571, 17917, 10698, 26525, 7033, 2003, 2183, 2000, 3000, 1012, 1996, 2493, 2024, 2183, 2000, 2047, 2259, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8a806",
   "metadata": {},
   "source": [
    "To convert the IDs to string tokens, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4dba3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1153421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'our', 'friend', 'gr', '##ung', '##le', 'sv', '##oni', '##tov', '##ich', 'is', 'going', 'to', 'paris', '.', 'the', 'students', 'are', 'going', 'to', 'new', 'york', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cae64d",
   "metadata": {},
   "source": [
    "Converting the tokens back to a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6e7bf07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] our friend grungle svonitovich is going to paris . the students are going to new york . [SEP]'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab6d65e",
   "metadata": {},
   "source": [
    "### BERT Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "513782f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e958d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f7f1dd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our',\n",
       " 'friend',\n",
       " 'gr',\n",
       " '##ung',\n",
       " '##le',\n",
       " 'sv',\n",
       " '##oni',\n",
       " '##tov',\n",
       " '##ich',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'paris',\n",
       " '.',\n",
       " 'the',\n",
       " 'students',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'new',\n",
       " 'york',\n",
       " '.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c21459d",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02de407",
   "metadata": {},
   "source": [
    "### Tiktoken\n",
    "\n",
    "Tiktoken is an OpenAI module that offers fast and efficient BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7917d72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c99ec",
   "metadata": {},
   "source": [
    "Load a specific encoding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c33b63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"o200k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce026f5b",
   "metadata": {},
   "source": [
    "Load the encoding for a specific OpenAI model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe431ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fbf4934c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Encoding 'o200k_base'>\n"
     ]
    }
   ],
   "source": [
    "print(enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4416bc4",
   "metadata": {},
   "source": [
    "Encoding models are translators between text and the model's internal representation or language. The models cl100k_base and o200k_base differ by:\n",
    "\n",
    "| Feature | o200k_base (Newer) | cl100k_base (Older) |\n",
    "| --- | --- | --- |\n",
    "| Associated Models | GPT-4o, GPT-4o-mini | GPT-4, GPT-3.5-Turbo, Text-Embedding-Ada-002 |\n",
    "| Vocabulary Size | approx. 200,000 tokens | approx. 100,000 tokens |\n",
    "| Token Compression | Significantly higher and more efficient. | Good, but less efficient than o200k_base. |\n",
    "| Multilingual Support | Highly optimized for non-English and non-Latin scripts (e.g., Tamil, Chinese, Japanese). | Less efficient for many non-English languages, often splitting diacritics and letters into more tokens. |\n",
    "| Tokenization Rules | Features a major upgrade in the regex pattern for handling word boundaries, specifically to better group diacritics (â, ê, î) and other Unicode character categories. | Uses an older, less sophisticated regex pattern for word boundaries. |\n",
    "| Cost &amp; Performance | Generally results in a lower token count for the same body of text (especially non-English), which can lead to lower API costs and faster processing. | Results in a higher token count for non-English texts, potentially increasing cost and context window usage. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efe377b",
   "metadata": {},
   "source": [
    "There are a couple of different encoding models in titoken:\n",
    "\n",
    "| Encoding Name | Associated OpenAI Models (Examples) |\n",
    "| --- | --- |\n",
    "| o200k_base    | gpt-4o, gpt-4o-mini, gpt-4.5-*, gpt-4.1-* (Newer models) |\n",
    "| cl100k_base   | gpt-4, gpt-3.5-turbo, text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large (Newer GPT and Embedding models) |\n",
    "| p50k_base     | Codex models (for code), text-davinci-002, text-davinci-003 |\n",
    "| r50k_base     | GPT-3 models (like davinci, curie, babbage, ada), also often referred to as gpt2 |\n",
    "| p50k_edit     | Older edit models like text-davinci-edit-001, code-davinci-edit-001 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e3798",
   "metadata": {},
   "source": [
    "We can look up the models for a specific encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a741ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3efe3a",
   "metadata": {},
   "source": [
    "Check whether the model encodes and decodes the text correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f21a49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462803e",
   "metadata": {},
   "source": [
    "Tokenize the following text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0f4f5c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_tokenize = \"Our friend Grungle Svonitovich is going to Paris. The students are going to New York.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52075cce",
   "metadata": {},
   "source": [
    "Get the IDs for the individual tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "98b023f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = enc.encode(text_to_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d4c5b079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7942, 5168, 2502, 44361, 42625, 263, 278, 106793, 382, 2966, 316, 12650, 13, 623, 4501, 553, 2966, 316, 2036, 6175, 13]\n"
     ]
    }
   ],
   "source": [
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25abd43",
   "metadata": {},
   "source": [
    "Get the strings for the token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b523788",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_strings = [enc.decode_single_token_bytes(token_id) for token_id in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1e00ba1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our', ' friend', ' Gr', 'ungle', ' Sv', 'on', 'it', 'ovich', ' is', ' going', ' to', ' Paris', '.', ' The', ' students', ' are', ' going', ' to', ' New', ' York', '.']\n"
     ]
    }
   ],
   "source": [
    "print([ x.decode('utf-8') for x in token_strings ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821f6dfc",
   "metadata": {},
   "source": [
    "We can print out the tokens by their ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5eeece4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7942:\t 'Our'\n",
      "5168:\t ' friend'\n",
      "2502:\t ' Gr'\n",
      "44361:\t 'ungle'\n",
      "42625:\t ' Sv'\n",
      "263:\t 'on'\n",
      "278:\t 'it'\n",
      "106793:\t 'ovich'\n",
      "382:\t ' is'\n",
      "2966:\t ' going'\n",
      "316:\t ' to'\n",
      "12650:\t ' Paris'\n",
      "13:\t '.'\n",
      "623:\t ' The'\n",
      "4501:\t ' students'\n",
      "553:\t ' are'\n",
      "2966:\t ' going'\n",
      "316:\t ' to'\n",
      "2036:\t ' New'\n",
      "6175:\t ' York'\n",
      "13:\t '.'\n"
     ]
    }
   ],
   "source": [
    "for t in zip(token_ids, token_strings):\n",
    "    print(f\"{t[0]}:\\t '{t[1].decode('utf-8')}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72932dab",
   "metadata": {},
   "source": [
    "&copy; 2025-2026 by [Damir Cavar](http://damir.cavar.me/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
