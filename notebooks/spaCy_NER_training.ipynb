{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Named Entity Annotation using Regular Expressions\n",
    "\n",
    "(C) 2024 by [Damir Cavar](http://damir.cavar.me/)\n",
    "\n",
    "**Version:** 0.1, November 2024\n",
    "\n",
    "**Download:** This and various other Jupyter notebooks are available from my [GitHub repo](https://github.com/dcavar/python-tutorial-for-ipython).\n",
    "\n",
    "The following code shows how simple regular expression matches can be converted to spaCy Named Entity labels. The goal is to provide a first level annotation of entities, manually correct the annotations, and generate a corpus for training neural annotation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banks = \"\"\"\n",
    "Bank of America\n",
    "Bank of America Corporation\n",
    "Citigroup\n",
    "Citigroup Inc.\n",
    "Citi\n",
    "Goldman Sachs\n",
    "The Goldman Sachs Group, Inc.\n",
    "Goldman Sachs Group, Inc.\n",
    "JP Morgan\n",
    "JPMorgan Chase\n",
    "JPMorgan Chase & Co.\n",
    "JPMorganChase\n",
    "Morgan Stanley\n",
    "The PNC Financial Services Group, Inc.\n",
    "PNC Financial Services Group, Inc.\n",
    "PNC Bank\n",
    "U.S. Bancorp\n",
    "Wells Fargo\n",
    "Wells Fargo & Company\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurances = \"\"\"\n",
    "Farmers Insurance Group\n",
    "Farmers\n",
    "Acuity Insurance\n",
    "Aflac\n",
    "Aflac Incorporated\n",
    "Allianz Life\n",
    "Allied Insurance\n",
    "Allstate\n",
    "The Allstate Corporation\n",
    "American Automobile Association\n",
    "AAA\n",
    "American Family Insurance\n",
    "American Income Life Insurance Company\n",
    "AIL\n",
    "American International Group\n",
    "AIG\n",
    "Government Employees Insurance Company\n",
    "GEICO\n",
    "Liberty Mutual\n",
    "Liberty Mutual Insurance Company\n",
    "Zurich Insurance Group\n",
    "Zurich Insurance Group Ltd\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter out regular expression operators and convert them to symbols, as here for the period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banks = banks.replace(r'.', r'\\.')\n",
    "insurances = insurances.replace(r'.', r'\\.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove empty elements from the list by filtering in the list comprehension. Duplicates are removed by converting the list to a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banks_list = { x for x in banks.splitlines() if x }\n",
    "insurances_list = { x for x in insurances.splitlines() if x }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lists can now be converted to labeled groups in Python regular expressions. We will use the labels as Named Entity tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regular_expression = re.compile( r\"|\".join( (r\"(?P<BANK>\" + r\"|\".join( banks_list ) + r\")\", \n",
    "\t\t\t\t\t\t\t\t\t\t\t r\"(?P<INSURANCE>\" + r\"|\".join( insurances_list ) + r\")\") ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some sample text that we want to annotate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Zurich Insurance Group Ltd is a Swiss insurance company, headquartered in ZÃ¼rich, and the country's largest insurer.\n",
    "Wells Fargo is an American multinational financial services company with a significant global presence.\n",
    "JPMorgan Chase & Co. (stylized as JPMorganChase) is an American multinational financial services firm headquartered in New York City and incorporated in Delaware. It is the largest bank in the United States and the world's largest bank by market capitalization as of 2023.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "for match in regular_expression.finditer(sample_text):\n",
    "\tfor label, value in match.groupdict().items():\n",
    "\t\tif value:\n",
    "\t\t\tbreak\n",
    "\tprint(f\"{label}: {match.start()} {match.end()} {match.group(0)}\")\n",
    "\tif label:\n",
    "\t\tannotations.append( (match.start(), match.end(), label) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run the text through the spaCy NLP pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sample_text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can process sentence by sentence and generate the entity annotations for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "for sentence in doc.sents:\n",
    "\tannotations = []\n",
    "\tfor match in regular_expression.finditer(sentence.text):\n",
    "\t\tfor label, value in match.groupdict().items():\n",
    "\t\t\tif value:\n",
    "\t\t\t\tbreak\n",
    "\t\tif label:\n",
    "\t\t\tannotations.append( (match.start(), match.end(), label) )\n",
    "\tif annotations:\n",
    "\t\ttraining_data.append( (sentence.text, { 'entities': annotations }) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting data structure is a list of tuples. The first element is the sentence text. The second is a dictionary with a key `entities` that has a list of antity annotation tuples as value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in training_data:\n",
    "\tprint(x[0])\n",
    "\tprint(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_new = spacy.blank(\"xx\")  # create blank Language class\n",
    "nlp_new.add_pipe('sentencizer')\n",
    "ner = nlp_new.add_pipe(\"ner\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, annotations in training_data:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_new.begin_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "other_pipes = [pipe for pipe in nlp_new.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nlp_new.disable_pipes(*other_pipes):  # only train NER\n",
    "    for itn in range(100):\n",
    "        random.shuffle(training_data)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(training_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            for text, annotations in batch:\n",
    "                print(text)\n",
    "                print(annotations)\n",
    "                doc = nlp_new.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                nlp_new.update([example],\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, _ in training_data:\n",
    "    doc = nlp_new(text)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"./models_ner/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "nlp_new.to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_test = spacy.load(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, _ in training_data:\n",
    "    doc = nlp_test(text)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    #print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(C) 2024 by [Damir Cavar](http://damir.cavar.me/) <<dcavar@iu.edu>>**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
