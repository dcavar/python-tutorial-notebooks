{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCFG Parsing using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(C) 2018-2025 by [Damir Cavar](http://damir.cavar.me/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download:** This and various other Jupyter notebooks are available from my [GitHub repo](https://github.com/dcavar/python-tutorial-notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/) ([CA BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install all thre NLTK data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or install just the Penn treebank portion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m nltk.downloader treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on [my](http://damir.cavar.me/) own NLTK notebooks and [Michael Elhadad](https://www.cs.bgu.ac.il/~elhadad/)'s notebook *[Constituent-based Syntactic Parsing with NLTK](https://www.cs.bgu.ac.il/~elhadad/nlp16/nltk-pcfg.html)*.  See for related material:\n",
    "\n",
    "1. [Python Parsing with NLTK](https://github.com/dcavar/python-tutorial-for-ipython/blob/master/notebooks/Python%20Parsing%20with%20NLTK.ipynb)\n",
    "1. [Python Parsing with NLTK and Foma](https://github.com/dcavar/python-tutorial-for-ipython/blob/master/notebooks/Python%20Parsing%20with%20NLTK%20and%20Foma.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial related to the discussion of grammar engineering and parsing in the class Alternative Syntactic Theories and [Advanced Natural Language Processing](http://damir.cavar.me/l645/) taught at [Indiana University at Bloomington](https://bloomington.iu.edu/) in Spring 2017 and Fall 2018, 2019, 2020, 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples presuppose an installed Python 3.x [NLTK](https://www.nltk.org/) module with all the dependent modules and packages, as well as the [data set for NLTK](https://www.nltk.org/data.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Penn-Treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the [NLTK](https://www.nltk.org/) *corpus* module to read and use the different corpora in the NLTK [data set](https://www.nltk.org/data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the portion of the Penn Treebank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ptb = nltk.corpus.treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus consists of a collection of files. Each file has an ID (file name). You can print the IDs using the *fileids* method, selecting the top 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ptb.fileids()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the content of a particular file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ptb.raw('wsj_0001.mrg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the file `wsj_0001.mrg` contains two sentence trees. You can access each sentence tree individually. Here we print out the first sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ptb.parsed_sents('wsj_0001.mrg')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in the following code we print the second tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ptb.parsed_sents('wsj_0001.mrg')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the tree structure as a graphic, you can apply the *draw()* method to the particular tree. The following code will open up a window with the drawing of the first tree in the file *wsj_0001.mrg*. This will pop up a window that might be hidden behind your notebook view in the browser!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ptb.parsed_sents('wsj_0001.mrg')[0].draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the menu of the window you can export the drawing of the tree as a Postscript file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NLTK](https://www.nltk.org/) allows you to define your own tree using the bracketed notation and the *[Tree](http://www.nltk.org/_modules/nltk/tree.html)* class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a string variable with a syntactic tree and create a Tree object by applying the *fromstring* method from the *Tree* class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trees = \"(S (NP (NN cats ) ) (VP (V chase ) (NP (NN mice ) ) ))\"\n",
    "myTree = Tree.fromstring(trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the tree in bracketed notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(myTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internal representation of the tree looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(myTree.__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree can be written in LaTeX format for the *qtree* package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(myTree.pformat_latex_qtree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw the tree by applying the *draw()* method to the tree object. The following code will open up a window with the drawing of the corresponding tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myTree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the label of a tree or subtrees using the *label* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(myTree.label())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A daughter of the root node of the tree can be accessed using the list index notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(myTree[0])\n",
    "print(myTree[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the label of these subtrees by applying the *label* method to them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(myTree[0].label())\n",
    "print(myTree[1].label())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can traverse the tree using these bracketed index operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(myTree[0])\n",
    "print(myTree[0,0])\n",
    "print(myTree[0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sections of the tree can be modified or replaced using the index notation. We replace the subject *(NN cats)* with *(NN dogs)* in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myTree[0,0] = Tree.fromstring('(NN dogs)')\n",
    "print(myTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various details about the tree can be accessed. We can extract the leaves of the tree using the *leaves* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(myTree.leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query the height of a tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(myTree.height())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For specific purposes and some specific algorithms one might want to convert the tree to remove all unary relations between symbols. To collapse for example *(NP (NP (NN dogs )))* to *(NP+NP (NN cats ))*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trees2 = \"(S (NP (NP (NN cats ) ) ) (VP (V chase ) (NP (NN mice ) ) ))\"\n",
    "myTree2 = Tree.fromstring(trees2)\n",
    "myTree2.collapse_unary()\n",
    "print(myTree2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert a tree to Chomsky Normal Form as well. In the following tree we have a subject NP branching into three daughter nodes. This is converted to a binary branching structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trees2 = \"(S (NP (NP (DET the ) (JJ big ) (NN cats ) ) ) (VP (V chase ) (NP (NN mice ) ) ))\"\n",
    "myTree2 = Tree.fromstring(trees2)\n",
    "myTree2.chomsky_normal_form()\n",
    "print(myTree2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate all production rules for a tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myTree.productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees can contain nodes that are complex objects. Nodes do not have to be strings. In the following code we replace the root node of our tree with a tuple of string and integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myTree.set_label( ('S', 3) )\n",
    "print(myTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic rules or trees can be defined in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pt = nltk.tree.ProbabilisticTree('NP', ['DET', 'N'], prob=0.5)\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous sections (see links to the other notebooks above), we have seen how CFGs can be defined, read from a file, or used in a parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCFG rules are defined in the same way. In addition a probability is assigned to each right-hand side of a rule. All probabilities for one particular left-hand side have to sum up to 1. The following code imports the PCFG class from NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import PCFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pcfg1 = PCFG.fromstring(\"\"\"\n",
    "    S -> NP VP [1.0]\n",
    "    NP -> Det N [0.5]\n",
    "    NP -> NP PP [0.25]\n",
    "    NP -> N [0.25]\n",
    "    PP -> P NP [1.0]\n",
    "    VP -> VP PP [0.1] | V NP [0.7] | V [0.2]\n",
    "    N -> 'woman' [0.3] | 'man' [0.3] | 'telescope' [0.3] | 'mixer' [0.1]\n",
    "    Det -> 'the' [0.6] | 'a' [0.2] | 'my' [0.2]\n",
    "    V -> 'killed' [0.35] | 'saw' [0.65]\n",
    "    P -> 'with' [0.61] | 'under' [0.39]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the productions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pcfg1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import the portion of the Penn Treebank as *treebank*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now loop over all files, read the sentences in, extract all production rules from them, and aggregate the production rules in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "productions = []\n",
    "for t in treebank.fileids():\n",
    "    for x in treebank.parsed_sents(t):\n",
    "        productions += x.productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are printing the last 20 in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(productions[-20:])\n",
    "for i in range(10):\n",
    "    print(productions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point one could remove some productions from the collection or change them according to some goal. We could also import *Nonterminal* and *Production* from *NLTK* to create new production rules and tweak the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import Nonterminal, Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a new *Production* object, it is necessary to use the *Nonterminal* object for symbols. The right-hand-side of production rules has to be a list or tuple. Terminals are simply strings, as in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = Production(Nonterminal('NNP'), [\"Seung\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the correct declaration of the production rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now append the new production rule to the list of productions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "productions.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The appended production is now last in the list. We are printing out the last 20 productions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(productions[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manipulate the probabilities, one could tweak the probability mass for the left-hand-side symbol, or simply append more such production rules before computing the frequencies and maximum likelihood estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTree = Tree.fromstring(\"(S (NP (NP (DET the ) (JJ big ) (NN cats ) ) ) (VP (V chase ) (NP (NN mice ) ) ))\")\n",
    "p1 = myTree.productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The production rules are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can concatenate these productions with the existing productions from the Penn Treebank using a *+* operator in the two production lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "productions = productions + p1\n",
    "print(productions[-30:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to induce the PCFG, we need to define a *Nonterminal* object with the start symbol as label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import Nonterminal\n",
    "S = Nonterminal('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now induce the PCFG using the *Nonterminal* start symbol and the list of production rules. We are printing out the first 20 productions here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import induce_pcfg\n",
    "grammar = induce_pcfg(S, productions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can normalize the trees by collapsing unary branches and converting them to Chomsky Normal Form before we extract the production rules and induce the grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "productions = []\n",
    "for t in treebank.fileids():\n",
    "    for x in treebank.parsed_sents(t):\n",
    "        #x.collapse_unary(collapsePOS = False)\n",
    "        #x.chomsky_normal_form(horzMarkov = 2)\n",
    "        productions += x.productions()\n",
    "#productions.append(\"N -> tie\")\n",
    "#productions.append(\"V -> killed\")\n",
    "myTree = Tree.fromstring(\"(S (NP (DET she ) ) (VP (V killed ) (NP (DET the ) (NN man ) (PP (P with ) (NP (DET the ) (N tie ) ) ) ) ) )\")\n",
    "productions += myTree.productions()\n",
    "myTree = Tree.fromstring(\"(S (NP (DET she ) ) (VP (V killed ) (NP (DET the ) (NN man ) ) (PP (P with ) (NP (DET the ) (N tie ) ) ) ) )\")\n",
    "productions += myTree.productions()\n",
    "grammar = induce_pcfg(S, productions)\n",
    "print(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing with PCFGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides various parser implementations. One that implements the Viterbi CKY n-best parses over a PCFG is available in the [parse.viterbi](http://www.nltk.org/_modules/nltk/parse/viterbi.html) module. (See [Michael Elhadad](https://www.cs.bgu.ac.il/~elhadad/)'s notebook *[Constituent-based Syntactic Parsing with NLTK](https://www.cs.bgu.ac.il/~elhadad/nlp16/nltk-pcfg.html)* for more details.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Parse sentence using induced grammar:\")\n",
    "\n",
    "parser.trace(3)\n",
    "\n",
    "# sent = treebank.parsed_sents('wsj_0001.mrg')[0].leaves()\n",
    "#sent = \"She killed the man with the tie\".split()\n",
    "sent = \"John saw Peter .\".split()\n",
    "print(sent)\n",
    "\n",
    "from nltk.parse import ViterbiParser, InsideChartParser\n",
    "#parser = nltk.pchart.InsideChartParser(grammar)\n",
    "parser = ViterbiParser(grammar)\n",
    "# parser = InsideChartParser(grammar)\n",
    "\n",
    "for parse in parser.parse_all(sent):\n",
    "    print(parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other parsers are defined in the *nltk.parse* module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time\n",
    "from nltk import tokenize\n",
    "#from nltk.grammar import Nonterminal,  toy_pcfg1\n",
    "from nltk.parse import pchart\n",
    "from nltk.parse import ViterbiParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcfg_rules = \"\"\"\n",
    "S -> NP VP\n",
    "S -> NP VP\n",
    "S -> NP VP\n",
    "NP -> ART N\n",
    "NP -> ART N\n",
    "NP -> ART N\n",
    "NP -> ART ADJ N\n",
    "NP -> ART ADJ N\n",
    "NP -> ADJ N\n",
    "NP -> ART N PP\n",
    "NP -> ART N PP\n",
    "NP -> ART N PP\n",
    "NP -> ART N PP\n",
    "NP -> N PP\n",
    "NP -> ART N S_REL\n",
    "S_REL -> RELP S\n",
    "NP -> PRON\n",
    "PP -> P NP\n",
    "VP -> V NP\n",
    "VP -> V\n",
    "VP -> V NP\n",
    "VP -> V NP\n",
    "VP -> V NP NP\n",
    "VP -> V NP PP\n",
    "\"\"\".split('\\n')\n",
    "pcfg_lexicon = \"\"\"\n",
    "RELP -> that\n",
    "ART -> the\n",
    "ART -> a\n",
    "ART -> my\n",
    "N -> John\n",
    "N -> tie\n",
    "N -> man\n",
    "N -> woman\n",
    "N -> telescope\n",
    "V -> saw\n",
    "V -> killed\n",
    "P -> with\n",
    "P -> in\n",
    "PRON -> I\n",
    "PRON -> she\n",
    "PRON -> he\n",
    "ADJ -> big\n",
    "ADJ -> red\n",
    "\"\"\".split('\\n')\n",
    "rules = [ (z[0].strip(), z[1].strip()) for z in [ y.split('->') for y in [ x for x in pcfg_rules if x.strip() ] ] ]\n",
    "productions = [ Production(Nonterminal(p[0]), [ Nonterminal(x) for x in p[1].split() ]) for p in rules ]\n",
    "lexicon = [ (z[0].strip(), z[1].strip()) for z in [ y.split('->') for y in [ x for x in pcfg_lexicon if x.strip() ] ] ]\n",
    "productions += [ Production(Nonterminal(p[0]), [ p[1] ]) for p in lexicon ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toy_pcfg1 = nltk.grammar.induce_pcfg(Nonterminal(\"S\"), productions)\n",
    "\n",
    "demos = [('I saw the man with the telescope', toy_pcfg1)]\n",
    "sent, grammar = demos[0]\n",
    "\n",
    "# Tokenize the sentence.\n",
    "tokens = sent.split()\n",
    "\n",
    "# Define a list of parsers.  We'll use all parsers.\n",
    "parsers = [\n",
    "ViterbiParser(grammar),\n",
    "pchart.InsideChartParser(grammar),\n",
    "pchart.RandomChartParser(grammar),\n",
    "pchart.UnsortedChartParser(grammar),\n",
    "pchart.LongestChartParser(grammar),\n",
    "pchart.InsideChartParser(grammar, beam_size = len(tokens)+1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now loop over the different parsers and compare the output and performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the parsers on the tokenized sentence.\n",
    "from functools import reduce\n",
    "times = []\n",
    "average_p = []\n",
    "num_parses = []\n",
    "all_parses = {}\n",
    "for parser in parsers:\n",
    "    print('\\ns: %s\\nparser: %s\\ngrammar: %s' % (sent,parser,grammar))\n",
    "    parser.trace(3)\n",
    "    t = time.time()\n",
    "    parses = parser.parse_all(tokens)\n",
    "    times.append(time.time()-t)\n",
    "    if parses: \n",
    "        lp = len(parses)\n",
    "        p = reduce(lambda a,b:a+b.prob(), parses, 0.0)\n",
    "    else: \n",
    "        p = 0\n",
    "    average_p.append(p)\n",
    "    num_parses.append(len(parses))\n",
    "    for p in parses: \n",
    "        all_parses[p.freeze()] = 1\n",
    "\n",
    "# Print summary statistics\n",
    "print()\n",
    "print('-------------------------+------------------------------------------')\n",
    "print('   Parser           Beam | Time (secs)   # Parses   Average P(parse)')\n",
    "print('-------------------------+------------------------------------------')\n",
    "for i in range(len(parsers)):\n",
    "    print('%19s %4d |%11.4f%11d%19.14f' % (parsers[i].__class__.__name__,\n",
    "      getattr(parsers[0], \"beam_size\", 0),\n",
    "      times[i], \n",
    "      num_parses[i], \n",
    "      average_p[i]))\n",
    "parses = all_parses.keys()\n",
    "if parses: \n",
    "    p = reduce(lambda a,b:a+b.prob(), parses, 0)/len(parses)\n",
    "else: \n",
    "    p = 0\n",
    "print('-------------------------+------------------------------------------')\n",
    "print('%19s      |%11s%11d%19.14f' % ('(All Parses)', 'n/a', len(parses), p))\n",
    "print()\n",
    "\n",
    "for parse in parses:\n",
    "    print(parse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C) 2018-2025 by [Damir Cavar](http://damir.cavar.me/) - [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/) ([CA BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)). Parts of the code are taken from [Michael Elhadad](https://www.cs.bgu.ac.il/~elhadad/)'s notebook *[Constituent-based Syntactic Parsing with NLTK](https://www.cs.bgu.ac.il/~elhadad/nlp16/nltk-pcfg.html)*. Please consult him for details about the copyright."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
