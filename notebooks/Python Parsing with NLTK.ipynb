{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Parsing with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(C) 2017-2019 by [Damir Cavar](http://damir.cavar.me/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/) ([CA BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial related to the discussion of grammar engineering and parsing in the class *Alternative Syntactic Theories* and *Advanced Natural Language Processing* taught at Indiana University in Spring 2017 and Fall 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following examples are taken from the NLTK [parsing HOWTO](http://www.nltk.org/howto/parse.html) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Nonterminal, nonterminals, Production, CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt1 = Nonterminal('NP')\n",
    "nt2 = Nonterminal('VP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NP'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt1.symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt1 == Nonterminal('NP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt1 == nt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n"
     ]
    }
   ],
   "source": [
    "S, NP, VP, PP = nonterminals('S, NP, VP, PP')\n",
    "print(S.symbol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, V, P, DT = nonterminals('N, V, P, DT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod1 = Production(S, [NP, VP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod2 = Production(NP, [DT, NP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod1.lhs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NP, VP)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod1.rhs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod1 == Production(S, [NP, VP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod1 == prod2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = CFG.fromstring(\"\"\"\n",
    " S -> NP VP\n",
    " PP -> P NP\n",
    " PP -> P NP\n",
    " NP -> 'the' N | N PP | 'the' N PP\n",
    " VP -> V NP | V PP | V NP PP\n",
    " N -> 'cat'\n",
    " N -> 'fish'\n",
    " N -> 'dog'\n",
    " N -> 'rug'\n",
    " N -> 'mouse'\n",
    " V -> 'chased'\n",
    " V -> 'sat'\n",
    " P -> 'in'\n",
    " P -> 'on'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 18 productions (start state = S)\n",
      "    S -> NP VP\n",
      "    PP -> P NP\n",
      "    PP -> P NP\n",
      "    NP -> 'the' N\n",
      "    NP -> N PP\n",
      "    NP -> 'the' N PP\n",
      "    VP -> V NP\n",
      "    VP -> V PP\n",
      "    VP -> V NP PP\n",
      "    N -> 'cat'\n",
      "    N -> 'fish'\n",
      "    N -> 'dog'\n",
      "    N -> 'rug'\n",
      "    N -> 'mouse'\n",
      "    V -> 'chased'\n",
      "    V -> 'sat'\n",
      "    P -> 'in'\n",
      "    P -> 'on'\n"
     ]
    }
   ],
   "source": [
    "print(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can build complex feature structures using the following strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       [ GND = 'fem' ] ]\n",
      "[ AGR = [ NUM = 'pl'  ] ]\n",
      "[       [ PER = 3     ] ]\n",
      "[                       ]\n",
      "[ POS = 'N'             ]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "fstr = nltk.FeatStruct(\"[POS='N', AGR=[PER=3, NUM='pl', GND='fem']]\")\n",
    "print(fstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating shared paths is also possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ADDRESS = (1) [ NUMBER = 74           ] ]\n",
      "[               [ STREET = 'rue Pascal' ] ]\n",
      "[                                         ]\n",
      "[ NAME    = 'Lee'                         ]\n",
      "[                                         ]\n",
      "[ SPOUSE  = [ ADDRESS -> (1)  ]           ]\n",
      "[           [ NAME    = 'Kim' ]           ]\n"
     ]
    }
   ],
   "source": [
    "fstr2 = nltk.FeatStruct(\"\"\"[NAME='Lee', ADDRESS=(1)[NUMBER=74, STREET='rue Pascal'],\n",
    "                          SPOUSE=[NAME='Kim', ADDRESS->(1)]]\"\"\")\n",
    "print(fstr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create feature structures and try out unification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       [ GND = 'fem' ] ]\n",
      "[ AGR = [ NUM = 'pl'  ] ]\n",
      "[       [ PER = 3     ] ]\n",
      "[                       ]\n",
      "[ POS = 'N'             ]\n"
     ]
    }
   ],
   "source": [
    "fs1 = nltk.FeatStruct(\"[AGR=[PER=3, NUM='pl', GND='fem'], POS='N']\")\n",
    "fs2 = nltk.FeatStruct(\"[POS='N', AGR=[PER=3, GND='fem']]\")\n",
    "\n",
    "print(fs1.unify(fs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following examples are taken from the NLTK [parsing HOWTO](http://www.nltk.org/howto/parse.html) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sentence:\n",
      "I saw a dog\n",
      "['I', 'saw', 'a', 'dog']\n",
      "\n",
      "* Strategy: Bottom-up\n",
      "\n",
      "|.    I    .   saw   .    a    .   dog   .|\n",
      "|[---------]         .         .         .| [0:1] 'I'\n",
      "|.         [---------]         .         .| [1:2] 'saw'\n",
      "|.         .         [---------]         .| [2:3] 'a'\n",
      "|.         .         .         [---------]| [3:4] 'dog'\n",
      "|>         .         .         .         .| [0:0] NP -> * 'I'\n",
      "|[---------]         .         .         .| [0:1] NP -> 'I' *\n",
      "|>         .         .         .         .| [0:0] S  -> * NP VP\n",
      "|>         .         .         .         .| [0:0] NP -> * NP PP\n",
      "|[--------->         .         .         .| [0:1] S  -> NP * VP\n",
      "|[--------->         .         .         .| [0:1] NP -> NP * PP\n",
      "|.         >         .         .         .| [1:1] Verb -> * 'saw'\n",
      "|.         [---------]         .         .| [1:2] Verb -> 'saw' *\n",
      "|.         >         .         .         .| [1:1] VP -> * Verb NP\n",
      "|.         >         .         .         .| [1:1] VP -> * Verb\n",
      "|.         [--------->         .         .| [1:2] VP -> Verb * NP\n",
      "|.         [---------]         .         .| [1:2] VP -> Verb *\n",
      "|.         >         .         .         .| [1:1] VP -> * VP PP\n",
      "|[-------------------]         .         .| [0:2] S  -> NP VP *\n",
      "|.         [--------->         .         .| [1:2] VP -> VP * PP\n",
      "|.         .         >         .         .| [2:2] Det -> * 'a'\n",
      "|.         .         [---------]         .| [2:3] Det -> 'a' *\n",
      "|.         .         >         .         .| [2:2] NP -> * Det Noun\n",
      "|.         .         [--------->         .| [2:3] NP -> Det * Noun\n",
      "|.         .         .         >         .| [3:3] Noun -> * 'dog'\n",
      "|.         .         .         [---------]| [3:4] Noun -> 'dog' *\n",
      "|.         .         [-------------------]| [2:4] NP -> Det Noun *\n",
      "|.         .         >         .         .| [2:2] S  -> * NP VP\n",
      "|.         .         >         .         .| [2:2] NP -> * NP PP\n",
      "|.         [-----------------------------]| [1:4] VP -> Verb NP *\n",
      "|.         .         [------------------->| [2:4] S  -> NP * VP\n",
      "|.         .         [------------------->| [2:4] NP -> NP * PP\n",
      "|[=======================================]| [0:4] S  -> NP VP *\n",
      "|.         [----------------------------->| [1:4] VP -> VP * PP\n",
      "Nr edges in chart: 33\n",
      "(S (NP I) (VP (Verb saw) (NP (Det a) (Noun dog))))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nltk.parse.chart.demo(2, print_times=False, trace=1,\n",
    "                       sent='I saw a dog', numparses=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example how to apply top-down parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sentence:\n",
      "I saw John with Mary\n",
      "['I', 'saw', 'John', 'with', 'Mary']\n",
      "\n",
      "* Strategy: Top-down\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'Mary'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-39a14b6b42bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m nltk.parse.chart.demo(1, print_times=True, trace=0,\n\u001b[1;32m----> 2\u001b[1;33m                        sent='I saw John with Mary', numparses=2)\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\parse\\chart.py\u001b[0m in \u001b[0;36mdemo\u001b[1;34m(choice, print_times, print_grammar, print_trees, trace, sent, numparses)\u001b[0m\n\u001b[0;32m   1805\u001b[0m         \u001b[0mcp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChartParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrategies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1806\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1807\u001b[1;33m         \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchart_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1808\u001b[0m         \u001b[0mparses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\parse\\chart.py\u001b[0m in \u001b[0;36mchart_parse\u001b[1;34m(self, tokens, trace)\u001b[0m\n\u001b[0;32m   1447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1449\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1450\u001b[0m         \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_chart_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1451\u001b[0m         \u001b[0mgrammar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    682\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             raise ValueError(\n\u001b[1;32m--> 684\u001b[1;33m                 \u001b[1;34m\"Grammar does not cover some of the \"\u001b[0m \u001b[1;34m\"input words: %r.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m             )\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'Mary'\"."
     ]
    }
   ],
   "source": [
    "nltk.parse.chart.demo(1, print_times=True, trace=0,\n",
    "                       sent='I saw John with Mary', numparses=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how to apply bottom-up parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sentence:\n",
      "I saw John with a dog\n",
      "['I', 'saw', 'John', 'with', 'a', 'dog']\n",
      "\n",
      "* Strategy: Bottom-up\n",
      "\n",
      "Nr edges in chart: 53\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP (VP (Verb saw) (NP John)) (PP with (NP (Det a) (Noun dog)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP (Verb saw) (NP (NP John) (PP with (NP (Det a) (Noun dog))))))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nltk.parse.chart.demo(2, print_times=False, trace=0,\n",
    "                       sent='I saw John with a dog', numparses=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grammar with 18 productions (start state = S[])\n",
      "    S[] -> NP[] VP[]\n",
      "    PP[] -> Prep[] NP[]\n",
      "    NP[] -> NP[] PP[]\n",
      "    VP[] -> VP[] PP[]\n",
      "    VP[] -> Verb[] NP[]\n",
      "    VP[] -> Verb[]\n",
      "    NP[] -> Det[pl=?x] Noun[pl=?x]\n",
      "    NP[] -> 'John'\n",
      "    NP[] -> 'I'\n",
      "    Det[] -> 'the'\n",
      "    Det[] -> 'my'\n",
      "    Det[-pl] -> 'a'\n",
      "    Noun[-pl] -> 'dog'\n",
      "    Noun[-pl] -> 'cookie'\n",
      "    Verb[] -> 'ate'\n",
      "    Verb[] -> 'saw'\n",
      "    Prep[] -> 'with'\n",
      "    Prep[] -> 'under'\n",
      "\n",
      "* FeatureChartParser\n",
      "Sentence: I saw John with a dog\n",
      "|.I.s.J.w.a.d.|\n",
      "|[-] . . . . .| [0:1] 'I'\n",
      "|. [-] . . . .| [1:2] 'saw'\n",
      "|. . [-] . . .| [2:3] 'John'\n",
      "|. . . [-] . .| [3:4] 'with'\n",
      "|. . . . [-] .| [4:5] 'a'\n",
      "|. . . . . [-]| [5:6] 'dog'\n",
      "|[-] . . . . .| [0:1] NP[] -> 'I' *\n",
      "|[-> . . . . .| [0:1] S[] -> NP[] * VP[] {}\n",
      "|[-> . . . . .| [0:1] NP[] -> NP[] * PP[] {}\n",
      "|. [-] . . . .| [1:2] Verb[] -> 'saw' *\n",
      "|. [-> . . . .| [1:2] VP[] -> Verb[] * NP[] {}\n",
      "|. [-] . . . .| [1:2] VP[] -> Verb[] *\n",
      "|. [-> . . . .| [1:2] VP[] -> VP[] * PP[] {}\n",
      "|[---] . . . .| [0:2] S[] -> NP[] VP[] *\n",
      "|. . [-] . . .| [2:3] NP[] -> 'John' *\n",
      "|. . [-> . . .| [2:3] S[] -> NP[] * VP[] {}\n",
      "|. . [-> . . .| [2:3] NP[] -> NP[] * PP[] {}\n",
      "|. [---] . . .| [1:3] VP[] -> Verb[] NP[] *\n",
      "|. [---> . . .| [1:3] VP[] -> VP[] * PP[] {}\n",
      "|[-----] . . .| [0:3] S[] -> NP[] VP[] *\n",
      "|. . . [-] . .| [3:4] Prep[] -> 'with' *\n",
      "|. . . [-> . .| [3:4] PP[] -> Prep[] * NP[] {}\n",
      "|. . . . [-] .| [4:5] Det[-pl] -> 'a' *\n",
      "|. . . . [-> .| [4:5] NP[] -> Det[pl=?x] * Noun[pl=?x] {?x: False}\n",
      "|. . . . . [-]| [5:6] Noun[-pl] -> 'dog' *\n",
      "|. . . . [---]| [4:6] NP[] -> Det[-pl] Noun[-pl] *\n",
      "|. . . . [--->| [4:6] S[] -> NP[] * VP[] {}\n",
      "|. . . . [--->| [4:6] NP[] -> NP[] * PP[] {}\n",
      "|. . . [-----]| [3:6] PP[] -> Prep[] NP[] *\n",
      "|. . [-------]| [2:6] NP[] -> NP[] PP[] *\n",
      "|. [---------]| [1:6] VP[] -> VP[] PP[] *\n",
      "|. [--------->| [1:6] VP[] -> VP[] * PP[] {}\n",
      "|[===========]| [0:6] S[] -> NP[] VP[] *\n",
      "|. . [------->| [2:6] S[] -> NP[] * VP[] {}\n",
      "|. . [------->| [2:6] NP[] -> NP[] * PP[] {}\n",
      "|. [---------]| [1:6] VP[] -> Verb[] NP[] *\n",
      "|. [--------->| [1:6] VP[] -> VP[] * PP[] {}\n",
      "|[===========]| [0:6] S[] -> NP[] VP[] *\n",
      "(S[]\n",
      "  (NP[] I)\n",
      "  (VP[]\n",
      "    (VP[] (Verb[] saw) (NP[] John))\n",
      "    (PP[] (Prep[] with) (NP[] (Det[-pl] a) (Noun[-pl] dog)))))\n",
      "(S[]\n",
      "  (NP[] I)\n",
      "  (VP[]\n",
      "    (Verb[] saw)\n",
      "    (NP[]\n",
      "      (NP[] John)\n",
      "      (PP[] (Prep[] with) (NP[] (Det[-pl] a) (Noun[-pl] dog))))))\n"
     ]
    }
   ],
   "source": [
    "nltk.parse.featurechart.demo(print_times=False,\n",
    "                              print_grammar=True,\n",
    "                              parser=nltk.parse.featurechart.FeatureChartParser,\n",
    "                              sent='I saw John with a dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading grammars from files and editing them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the following NLTK modules in this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import CFG\n",
    "from nltk.grammar import FeatureGrammar as FCFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load a grammar from a file, that is located in the same folder as the current Jupyter notebook, in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = nltk.data.load('spanish1.cfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a ChartParser object with this grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp1 = nltk.parse.ChartParser(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *ChartParser* object has a parse-function that takes a list of tokens as a parameter. The token list can be generated using a language specific tokenizer. In this case we simply tokenize using the Python-function *split* on strings. The output of the parse function is a list of trees. We loop through the list of parse trees and print them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (SN (det las) (GN (nom_com mujeres)))\n",
      "  (SV (v adoran) (SN (det el) (GN (nom_prop Lucas)))))\n"
     ]
    }
   ],
   "source": [
    "for x in cp1.parse(\"las mujeres adoran el Lucas\".split()):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also edit a grammar directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg2 = CFG.fromstring(\"\"\"\n",
    " S -> NP VP\n",
    " PP -> P NP\n",
    " NP -> 'the' N | N PP | 'the' N PP\n",
    " VP -> V NP | V PP | V NP PP\n",
    " N -> 'cat'\n",
    " N -> 'dog'\n",
    " N -> 'bird'\n",
    " N -> 'rug'\n",
    " V -> 'chased'\n",
    " V -> 'sat'\n",
    " V -> 'bit'\n",
    " P -> 'in'\n",
    " P -> 'on'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parse our example sentences using the same approach as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp2 = nltk.parse.ChartParser(cfg2)\n",
    "for x in cp2.parse(\"the bird bit the dog\".split()):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example included a Context-free grammar. In the following example we load a Context-free Grammar with Features, instantiate a *FeatureChartParser*, and loop through the parse trees that are generated by our grammar to print them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcfg = nltk.data.load('spanish1.fcfg')\n",
    "fcp1 = nltk.parse.FeatureChartParser(fcfg)\n",
    "for x in fcp1.parse(u\"Miguel adorÃ³ el gato\".split()):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can edit a Feature CFG in the same way directly in this notebook and then parse with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcfg2 = FCFG.fromstring(\"\"\"\n",
    "% start CP\n",
    "# ############################\n",
    "# Grammar Rules\n",
    "# ############################\n",
    "CP -> Cbar[stype=decl]\n",
    "Cbar[stype=decl] -> IP[+TNS]\n",
    "IP[+TNS] -> DP[num=?n,pers=?p,case=nom] VP[num=?n,pers=?p]\n",
    "DP[num=?n,pers=?p,case=?k] ->  Dbar[num=?n,pers=?p,case=?k]\n",
    "Dbar[num=?n,pers=?p] -> D[num=?n,DEF=?d,COUNT=?c] NP[num=?n,pers=?p,DEF=?d,COUNT=?c]\n",
    "Dbar[num=?n,pers=?p] -> NP[num=?n,pers=?p,DEF=?d,COUNT=?c]\n",
    "Dbar[num=?n,pers=?p,case=?k] -> D[num=?n,pers=?p,+DEF,type=pron,case=?k]\n",
    "NP[num=?n,pers=?p,COUNT=?c] -> N[num=?n,pers=?p,type=prop,COUNT=?c]\n",
    "VP[num=?n,pers=?p] -> V[num=?n,pers=?p,val=1]\n",
    "VP[num=?n,pers=?p] -> V[num=?n,pers=?p,val=2] DP[case=acc]\n",
    "#\n",
    "# ############################\n",
    "# Lexical Rules\n",
    "# ############################\n",
    "D[-DEF,+COUNT,num=sg] -> 'a'\n",
    "D[-DEF,+COUNT,num=sg] -> 'an'\n",
    "D[+DEF] -> 'the'\n",
    "D[+DEF,gen=f,num=sg,case=nom,type=pron] -> 'she'\n",
    "D[+DEF,gen=m,num=sg,case=nom,type=pron] -> 'he'\n",
    "D[+DEF,gen=n,num=sg,type=pron] -> 'it'\n",
    "D[+DEF,gen=f,num=sg,case=acc,type=pron] -> 'her'\n",
    "D[+DEF,gen=m,num=sg,case=acc,type=pron] -> 'him'\n",
    "N[num=sg,pers=3,type=prop] -> 'John' | 'Sara' | 'Mary'\n",
    "V[tns=pres,num=sg,pers=3,val=2] -> 'loves' | 'calls' | 'sees' | 'buys'\n",
    "N[num=sg,pers=3,-COUNT] -> 'furniture' | 'air' | 'justice'\n",
    "N[num=sg,pers=3] -> 'cat' | 'dog' | 'mouse'\n",
    "N[num=pl,pers=3] -> 'cats' | 'dogs' | 'mice'\n",
    "V[tns=pres,num=sg,pers=3,val=1] -> 'sleeps' | 'snores'\n",
    "V[tns=pres,num=sg,pers=1,val=1] -> 'sleep' | 'snore'\n",
    "V[tns=pres,num=sg,pers=2,val=1] -> 'sleep' | 'snore'\n",
    "V[tns=pres,num=pl,val=1] -> 'sleep' | 'snore'\n",
    "V[tns=past,val=1] -> 'slept' | 'snored'\n",
    "V[tns=pres,num=sg,pers=3,val=2] -> 'calls' | 'sees' | 'loves'\n",
    "V[tns=pres,num=sg,pers=1,val=2] -> 'call' | 'see' | 'love'\n",
    "V[tns=pres,num=sg,pers=2,val=2] -> 'call' | 'see' | 'love'\n",
    "V[tns=pres,num=pl,val=2] -> 'call' | 'see' | 'love'\n",
    "V[tns=past,val=2] -> 'called' | 'saw' | 'loved'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a parser instance and parse with this grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcp2 = nltk.parse.FeatureChartParser(fcfg2, trace=1)\n",
    "sentence = \"John buys him\"\n",
    "result = list(fcp2.parse(sentence.split()))\n",
    "if result:\n",
    "    for x in result:\n",
    "        print(x)\n",
    "else:\n",
    "    print(\"*\", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countable nouns and articles in a DP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DPs and pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CP/IP sentence structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of the different Feature Parsers in NLTK.\n",
    "\n",
    "- nltk.parse.featurechart.FeatureChartParser\n",
    "- nltk.parse.featurechart.FeatureTopDownChartParser\n",
    "- nltk.parse.featurechart.FeatureBottomUpChartParser\n",
    "- nltk.parse.featurechart.FeatureBottomUpLeftCornerChartParser\n",
    "- nltk.parse.earleychart.FeatureIncrementalChartParser\n",
    "- nltk.parse.earleychart.FeatureEarleyChartParser\n",
    "- nltk.parse.earleychart.FeatureIncrementalTopDownChartParser\n",
    "- nltk.parse.earleychart.FeatureIncrementalBottomUpChartParser\n",
    "- nltk.parse.earleychart.FeatureIncrementalBottomUpLeftCornerChartParser\n",
    "\n",
    "I do not know whether this is an exhaustive list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C) 2017-2018 by [Damir Cavar](http://damir.cavar.me/) - [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/) ([CA BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
