{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Classification for Machine Learning for Computational Linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using token probabilities for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(C) 2017-2024 by [Damir Cavar](http://damir.cavar.me/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download:** This and various other Jupyter notebooks are available from my [GitHub repo](https://github.com/dcavar/python-tutorial-notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version:** 1.6, September 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/) ([CA BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial related to the discussion of a Bayesian classifier in the textbook [Machine Learning: The Art and Science of Algorithms that Make Sense of Data](https://www.cs.bris.ac.uk/~flach/mlbook/) by [Peter Flach](https://www.cs.bris.ac.uk/~flach/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial was developed as part of my course material for the course Machine Learning for Computational Linguistics in the [Computational Linguistics Program](http://cl.indiana.edu/) of the [Department of Linguistics](http://www.indiana.edu/~lingdept/) at [Indiana University](https://www.indiana.edu/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Training Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we have a set of e-mails that are annotated as *spam* or *ham*, as described in the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are $4$ e-mails labeled *ham* and $1$ e-mail is labeled *spam*, that is we have a total of $5$ texts in our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we would randomly pick an e-mail from the collection, the probability that we pick a spam e-mail would be $1 / 5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam emails might differ from ham e-mails just in some words. Here is a sample email constructed with typical keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = [ \"\"\"Our medicine cures baldness. No diagnostics needed.\n",
    "            We guarantee Fast Viagra delivery.\n",
    "            We can provide Human growth hormone. The cheapest Life\n",
    "            Insurance with us. You can Lose weight with this treatment.\n",
    "            Our Medicine now and No medical exams necessary.\n",
    "            Our Online pharmacy is the best.  This cream Removes\n",
    "            wrinkles and Reverses aging.\n",
    "            One treatment and you will Stop snoring.  We sell Valium\n",
    "            and Viagra.\n",
    "            Our Vicodin will help with Weight loss. Cheap Xanax.\"\"\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data structure above is a list of strings that contains only one string. The triple-double-quotes mark multi-line text. We can output the size of the variable *spam* this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a list of *ham* mails in a similar way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham = [ \"\"\"Hi Hans, hope to see you soon at our family party.\n",
    "           When will you arrive.\n",
    "           All the best to the family.\n",
    "           Sue\"\"\",\n",
    "      \"\"\"Dear Ata,\n",
    "         did you receive my last email related to the car insurance\n",
    "         offer? I would be happy to discuss the details with you.\n",
    "         Please give me a call, if you have any questions.\n",
    "         John Smith\n",
    "         Super Car Insurance\"\"\",\n",
    "      \"\"\"Hi everyone:\n",
    "         This is just a gentle reminder of today's first 2017 SLS\n",
    "         Colloquium, from 2.30 to 4.00 pm, in Ballantine 103.\n",
    "         Rodica Frimu will present a job talk entitled \"What is\n",
    "         so tricky in subject-verb agreement?\". The text of the\n",
    "         abstract is below.\n",
    "         If you would like to present something during the Spring,\n",
    "         please let me know.\n",
    "         The current online schedule with updated title\n",
    "         information and abstracts is available under:\n",
    "         http://www.iub.edu/~psyling/SLSColloquium/Spring2017.html\n",
    "         See you soon,\n",
    "         Peter\"\"\",\n",
    "      \"\"\"Dear Friends,\n",
    "         As our first event of 2017, the Polish Studies Center\n",
    "         presents an evening with artist and filmmaker Wojtek Sawa.\n",
    "         Please join us on JANUARY 26, 2017 from 5:30 p.m. to\n",
    "         7:30 p.m. in the Global and International Studies\n",
    "         Building room 1100 for a presentation by Wojtek Sawa\n",
    "         on his interactive  installation art piece The Wall\n",
    "         Speaksâ€“Voices of the Unheard. A reception will follow\n",
    "         the event where you will have a chance to meet the artist\n",
    "         and discuss his work.\n",
    "         Best,\"\"\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ham-mail list contains $4$ e-mails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ham))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access a particular e-mail via index from either spam or ham:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spam[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ham[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can lower-case the email using the string *lower* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ham[3].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can loop over all e-mails in spam or ham and lower-case the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ham:\n",
    "    print(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the tokenizer from NLTK to tokenize the lower-cased text into single tokens (words and punctuation marks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "print(word_tokenize(ham[0].lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the numer of tokens and types in lower-cased text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "myCounts = Counter(word_tokenize(\"This is a test. Will this test teach us how to count tokens?\".lower()))\n",
    "\n",
    "print(myCounts)\n",
    "print(\"number of  types:\", len(myCounts))\n",
    "print(\"number of tokens:\", sum(myCounts.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a frequency profile of ham and spam words given the two text collections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamFP = Counter()\n",
    "spamFP = Counter()\n",
    "\n",
    "for text in spam:\n",
    "    spamFP.update(word_tokenize(text.lower()))\n",
    "\n",
    "for text in ham:\n",
    "    hamFP.update(word_tokenize(text.lower()))\n",
    "\n",
    "print(\"Ham:\\n\",  hamFP)\n",
    "print(\"-\" * 30)\n",
    "print(\"Spam:\\n\", spamFP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "tokenlist = []\n",
    "frqprofiles = []\n",
    "for x in spam:\n",
    "    frqprofiles.append( Counter(word_tokenize(x.lower())) )\n",
    "    tokenlist.append( set(word_tokenize(x.lower())) )\n",
    "for x in ham:\n",
    "    frqprofiles.append( Counter(word_tokenize(x.lower())) )\n",
    "    tokenlist.append( set(word_tokenize(x.lower())) )\n",
    "#print(tokenlist)\n",
    "\n",
    "for x in frqprofiles[0]:\n",
    "    frq = frqprofiles[0][x]\n",
    "    counter = 0\n",
    "    for y in tokenlist:\n",
    "        if x in y:\n",
    "            counter += 1\n",
    "    print(x, frq * log(len(tokenlist)/counter, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that we pick randomly an e-mail that is spam or ham can be computed as the ratio of the counts divided by the number of e-mails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(spam) + len(ham)\n",
    "\n",
    "spamP = len(spam) / total\n",
    "hamP  = len(ham) / total\n",
    "\n",
    "print(\"probability to pick spam:\", spamP)\n",
    "print(\"probability to pick  ham:\", hamP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the total token count to calculate the relative frequency of the tokens, that is to generate likelihood estimates. We could *brute force* add one to create space in the probability mass for unknown tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalSpam = sum(spamFP.values()) + 1\n",
    "totalHam  = sum(hamFP.values()) + 1\n",
    "\n",
    "print(\"total spam counts + 1:\", totalSpam)\n",
    "print(\"total  ham counts + 1:\", totalHam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can relativize the counts in the frequency profiles now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamFP  = Counter( dict([ (token, frequency/totalHam)  for token, frequency in hamFP.items() ]) )\n",
    "spamFP = Counter( dict([ (token, frequency/totalSpam) for token, frequency in spamFP.items() ]) )\n",
    "\n",
    "print(hamFP)\n",
    "print(\"-\" * 30)\n",
    "print(spamFP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the default probability that we want to assign to unknown words as $1 / totalSpam$ or $1 / totalHam$ respectively. Whenever we encounter an unknown token that is not in our frequency profile, we will assign the default probability to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultSpam = 1 / totalSpam\n",
    "defaultHam  = 1 / totalHam\n",
    "\n",
    "print(\"default spam probability:\", defaultSpam)\n",
    "print(\"default  ham probability:\", defaultHam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test an unknown document by calculating how likely it was generated by the hamFP-distribution or the spamFP-distribution. We have to tokenize the lower-cased unknown document and compute the product of the likelihood of every single token in the text. We should scale this likelihood with the likelihood of randomly picking a ham or a spam e-mail. Let us calculate the likelihood that the random email is spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknownEmail = \"\"\"Dear ,\n",
    "we sell the cheapest and best Viagra on the planet. Our delivery is guaranteed confident and cheap.\n",
    "\"\"\"\n",
    "#unknownEmail = \"\"\"Dear Hans,\n",
    "#I have not seen you for so long. When will we go out for a coffee again.\n",
    "#\"\"\"\n",
    "\n",
    "tokens = word_tokenize(unknownEmail.lower())\n",
    "\n",
    "result = 1.0\n",
    "for token in tokens:\n",
    "    result *= spamFP.get(token, defaultSpam)\n",
    "\n",
    "print(result * spamP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this number is very small, a better strategy might be to sum up the log-likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "resultSpam = 0.0\n",
    "for token in tokens:\n",
    "    resultSpam += log(spamFP.get(token, defaultSpam), 2)\n",
    "resultSpam += log(spamP)\n",
    "\n",
    "print(resultSpam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultHam = 0.0\n",
    "for token in tokens:\n",
    "    resultHam += log(hamFP.get(token, defaultHam), 2)\n",
    "resultHam += log(hamP)\n",
    "\n",
    "print(resultHam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood for spam is larger than for *ham*. Our simple classifier would have guessed that this e-mail is *spam*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if max(resultHam, resultSpam) == resultHam:\n",
    "    print(\"e-mail is ham\")\n",
    "else:\n",
    "    print(\"e-mail is spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are numerous ways to improve the algorithm and tutorial. Please [send me](http://cavar.me/damir/) your suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C) 2017-2024 by [Damir Cavar](http://damir.cavar.me/) - [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/) ([CA BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "latex_metadata": {
   "affiliation": "Indiana University, Department of Linguistics, Bloomington, IN, USA",
   "author": "Damir Cavar",
   "title": "Bayesian Classification for Machine Learning for Computational Linguistics"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "1e28a5307a9b5c2fbeb0b263581f1cf3bfba9739188743f6a231f74c7de58892"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
