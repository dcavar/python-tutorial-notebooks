{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network Example\n",
    "\n",
    "Code taken from:\n",
    "\n",
    "https://pyimagesearch.com/2021/05/06/backpropagation-from-scratch-with-python/\n",
    "\n",
    "Backpropagation from scratch with Python\n",
    "by Adrian Rosebrock on May 6, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, alpha=0.1):\n",
    "        # initialize the list of weights matrices, then store the\n",
    "        # network architecture and learning rate\n",
    "        self.W = []\n",
    "        self.layers = layers\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # start looping from the index of the first layer but\n",
    "        # stop before we reach the last two layers\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            # randomly initialize a weight matrix connecting the\n",
    "            # number of nodes in each respective layer together,\n",
    "            # adding an extra node for the bias\n",
    "            w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
    "            self.W.append(w / np.sqrt(layers[i]))\n",
    "\n",
    "        # the last two layers are a special case where the input\n",
    "        # connections need a bias term but the output does not\n",
    "        w = np.random.randn(layers[-2] + 1, layers[-1])\n",
    "        self.W.append(w / np.sqrt(layers[-2]))\n",
    "\n",
    "    def __repr__(self):\n",
    "        # construct and return a string that represents the network\n",
    "        # architecture\n",
    "        return \"NeuralNetwork: {}\".format(\n",
    "            \"-\".join(str(l) for l in self.layers))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # compute and return the sigmoid activation value for a\n",
    "        # given input value\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_deriv(self, x):\n",
    "        # compute the derivative of the sigmoid function ASSUMING\n",
    "        # that x has already been passed through the 'sigmoid'\n",
    "        # function\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def fit(self, X, y, epochs=1000, displayUpdate=100):\n",
    "        # insert a column of 1's as the last entry in the feature\n",
    "        # matrix -- this little trick allows us to treat the bias\n",
    "        # as a trainable parameter within the weight matrix\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "\n",
    "        # loop over the desired number of epochs\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # loop over each individual data point and train\n",
    "            # our network on it\n",
    "            for (x, target) in zip(X, y):\n",
    "                self.fit_partial(x, target)\n",
    "\n",
    "            # check to see if we should display a training update\n",
    "            if epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(\"[INFO] epoch={}, loss={:.7f}\".format(\n",
    "                    epoch + 1, loss))\n",
    "\n",
    "    def fit_partial(self, x, y):\n",
    "        # construct our list of output activations for each layer\n",
    "        # as our data point flows through the network; the first\n",
    "        # activation is a special case -- it's just the input\n",
    "        # feature vector itself\n",
    "        A = [np.atleast_2d(x)]\n",
    "\n",
    "        # FEEDFORWARD:\n",
    "        # loop over the layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # feedforward the activation at the current layer by\n",
    "            # taking the dot product between the activation and\n",
    "            # the weight matrix -- this is called the \"net input\"\n",
    "            # to the current layer\n",
    "            net = A[layer].dot(self.W[layer])\n",
    "\n",
    "            # computing the \"net output\" is simply applying our\n",
    "            # nonlinear activation function to the net input\n",
    "            out = self.sigmoid(net)\n",
    "\n",
    "            # once we have the net output, add it to our list of\n",
    "            # activations\n",
    "            A.append(out)\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        # the first phase of backpropagation is to compute the\n",
    "        # difference between our *prediction* (the final output\n",
    "        # activation in the activations list) and the true target\n",
    "        # value\n",
    "        error = A[-1] - y\n",
    "\n",
    "        # from here, we need to apply the chain rule and build our\n",
    "        # list of deltas 'D'; the first entry in the deltas is\n",
    "        # simply the error of the output layer times the derivative\n",
    "        # of our activation function for the output value\n",
    "        D = [error * self.sigmoid_deriv(A[-1])]\n",
    "\n",
    "        # once you understand the chain rule it becomes super easy\n",
    "        # to implement with a 'for' loop -- simply loop over the\n",
    "        # layers in reverse order (ignoring the last two since we\n",
    "        # already have taken them into account)\n",
    "        for layer in np.arange(len(A) - 2, 0, -1):\n",
    "            # the delta for the current layer is equal to the delta\n",
    "            # of the *previous layer* dotted with the weight matrix\n",
    "            # of the current layer, followed by multiplying the delta\n",
    "            # by the derivative of the nonlinear activation function\n",
    "            # for the activations of the current layer\n",
    "            delta = D[-1].dot(self.W[layer].T)\n",
    "            delta = delta * self.sigmoid_deriv(A[layer])\n",
    "            D.append(delta)\n",
    "\n",
    "        # since we looped over our layers in reverse order we need to\n",
    "        # reverse the deltas\n",
    "        D = D[::-1]\n",
    "\n",
    "        # WEIGHT UPDATE PHASE\n",
    "        # loop over the layers\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # update our weights by taking the dot product of the layer\n",
    "            # activations with their respective deltas, then multiplying\n",
    "            # this value by some small learning rate and adding to our\n",
    "            # weight matrix -- this is where the actual \"learning\" takes\n",
    "            # place\n",
    "            self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
    "\n",
    "    def predict(self, X, addBias=True):\n",
    "        # initialize the output prediction as the input features -- this\n",
    "        # value will be (forward) propagated through the network to\n",
    "        # obtain the final prediction\n",
    "        p = np.atleast_2d(X)\n",
    "\n",
    "        # check to see if the bias column should be added\n",
    "        if addBias:\n",
    "            # insert a column of 1's as the last entry in the feature\n",
    "            # matrix (bias)\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    "\n",
    "        # loop over our layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # computing the output prediction is as simple as taking\n",
    "            # the dot product between the current activation value 'p'\n",
    "            # and the weight matrix associated with the current layer,\n",
    "            # then passing this value through a nonlinear activation\n",
    "            # function\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "\n",
    "        # return the predicted value\n",
    "        return p\n",
    "\n",
    "    def calculate_loss(self, X, targets):\n",
    "        # make predictions for the input data points then compute\n",
    "        # the loss\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "\n",
    "        # return the loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch=1, loss=0.5379580\n",
      "[INFO] epoch=100, loss=0.5001733\n",
      "[INFO] epoch=200, loss=0.4998304\n",
      "[INFO] epoch=300, loss=0.4996054\n",
      "[INFO] epoch=400, loss=0.4993540\n",
      "[INFO] epoch=500, loss=0.4988470\n",
      "[INFO] epoch=600, loss=0.4975845\n",
      "[INFO] epoch=700, loss=0.4939464\n",
      "[INFO] epoch=800, loss=0.4819670\n",
      "[INFO] epoch=900, loss=0.4468838\n",
      "[INFO] epoch=1000, loss=0.3891372\n",
      "[INFO] epoch=1100, loss=0.3323508\n",
      "[INFO] epoch=1200, loss=0.2748918\n",
      "[INFO] epoch=1300, loss=0.2228820\n",
      "[INFO] epoch=1400, loss=0.1883357\n",
      "[INFO] epoch=1500, loss=0.1676274\n",
      "[INFO] epoch=1600, loss=0.1538164\n",
      "[INFO] epoch=1700, loss=0.1392713\n",
      "[INFO] epoch=1800, loss=0.0756454\n",
      "[INFO] epoch=1900, loss=0.0267888\n",
      "[INFO] epoch=2000, loss=0.0169463\n",
      "[INFO] epoch=2100, loss=0.0126791\n",
      "[INFO] epoch=2200, loss=0.0102043\n",
      "[INFO] epoch=2300, loss=0.0085621\n",
      "[INFO] epoch=2400, loss=0.0073835\n",
      "[INFO] epoch=2500, loss=0.0064928\n",
      "[INFO] epoch=2600, loss=0.0057942\n",
      "[INFO] epoch=2700, loss=0.0052308\n",
      "[INFO] epoch=2800, loss=0.0047664\n",
      "[INFO] epoch=2900, loss=0.0043768\n",
      "[INFO] epoch=3000, loss=0.0040452\n",
      "[INFO] epoch=3100, loss=0.0037595\n",
      "[INFO] epoch=3200, loss=0.0035108\n",
      "[INFO] epoch=3300, loss=0.0032923\n",
      "[INFO] epoch=3400, loss=0.0030988\n",
      "[INFO] epoch=3500, loss=0.0029263\n",
      "[INFO] epoch=3600, loss=0.0027716\n",
      "[INFO] epoch=3700, loss=0.0026320\n",
      "[INFO] epoch=3800, loss=0.0025055\n",
      "[INFO] epoch=3900, loss=0.0023903\n",
      "[INFO] epoch=4000, loss=0.0022850\n",
      "[INFO] epoch=4100, loss=0.0021884\n",
      "[INFO] epoch=4200, loss=0.0020993\n",
      "[INFO] epoch=4300, loss=0.0020171\n",
      "[INFO] epoch=4400, loss=0.0019409\n",
      "[INFO] epoch=4500, loss=0.0018701\n",
      "[INFO] epoch=4600, loss=0.0018042\n",
      "[INFO] epoch=4700, loss=0.0017427\n",
      "[INFO] epoch=4800, loss=0.0016851\n",
      "[INFO] epoch=4900, loss=0.0016310\n",
      "[INFO] epoch=5000, loss=0.0015803\n",
      "[INFO] epoch=5100, loss=0.0015326\n",
      "[INFO] epoch=5200, loss=0.0014875\n",
      "[INFO] epoch=5300, loss=0.0014450\n",
      "[INFO] epoch=5400, loss=0.0014048\n",
      "[INFO] epoch=5500, loss=0.0013667\n",
      "[INFO] epoch=5600, loss=0.0013306\n",
      "[INFO] epoch=5700, loss=0.0012963\n",
      "[INFO] epoch=5800, loss=0.0012637\n",
      "[INFO] epoch=5900, loss=0.0012326\n",
      "[INFO] epoch=6000, loss=0.0012030\n",
      "[INFO] epoch=6100, loss=0.0011747\n",
      "[INFO] epoch=6200, loss=0.0011477\n",
      "[INFO] epoch=6300, loss=0.0011219\n",
      "[INFO] epoch=6400, loss=0.0010972\n",
      "[INFO] epoch=6500, loss=0.0010736\n",
      "[INFO] epoch=6600, loss=0.0010509\n",
      "[INFO] epoch=6700, loss=0.0010291\n",
      "[INFO] epoch=6800, loss=0.0010082\n",
      "[INFO] epoch=6900, loss=0.0009881\n",
      "[INFO] epoch=7000, loss=0.0009688\n",
      "[INFO] epoch=7100, loss=0.0009502\n",
      "[INFO] epoch=7200, loss=0.0009323\n",
      "[INFO] epoch=7300, loss=0.0009150\n",
      "[INFO] epoch=7400, loss=0.0008984\n",
      "[INFO] epoch=7500, loss=0.0008823\n",
      "[INFO] epoch=7600, loss=0.0008668\n",
      "[INFO] epoch=7700, loss=0.0008518\n",
      "[INFO] epoch=7800, loss=0.0008373\n",
      "[INFO] epoch=7900, loss=0.0008233\n",
      "[INFO] epoch=8000, loss=0.0008097\n",
      "[INFO] epoch=8100, loss=0.0007966\n",
      "[INFO] epoch=8200, loss=0.0007839\n",
      "[INFO] epoch=8300, loss=0.0007715\n",
      "[INFO] epoch=8400, loss=0.0007596\n",
      "[INFO] epoch=8500, loss=0.0007480\n",
      "[INFO] epoch=8600, loss=0.0007367\n",
      "[INFO] epoch=8700, loss=0.0007258\n",
      "[INFO] epoch=8800, loss=0.0007152\n",
      "[INFO] epoch=8900, loss=0.0007048\n",
      "[INFO] epoch=9000, loss=0.0006948\n",
      "[INFO] epoch=9100, loss=0.0006851\n",
      "[INFO] epoch=9200, loss=0.0006756\n",
      "[INFO] epoch=9300, loss=0.0006663\n",
      "[INFO] epoch=9400, loss=0.0006573\n",
      "[INFO] epoch=9500, loss=0.0006486\n",
      "[INFO] epoch=9600, loss=0.0006400\n",
      "[INFO] epoch=9700, loss=0.0006317\n",
      "[INFO] epoch=9800, loss=0.0006236\n",
      "[INFO] epoch=9900, loss=0.0006157\n",
      "[INFO] epoch=10000, loss=0.0006080\n",
      "[INFO] epoch=10100, loss=0.0006005\n",
      "[INFO] epoch=10200, loss=0.0005931\n",
      "[INFO] epoch=10300, loss=0.0005860\n",
      "[INFO] epoch=10400, loss=0.0005790\n",
      "[INFO] epoch=10500, loss=0.0005721\n",
      "[INFO] epoch=10600, loss=0.0005654\n",
      "[INFO] epoch=10700, loss=0.0005589\n",
      "[INFO] epoch=10800, loss=0.0005525\n",
      "[INFO] epoch=10900, loss=0.0005463\n",
      "[INFO] epoch=11000, loss=0.0005402\n",
      "[INFO] epoch=11100, loss=0.0005342\n",
      "[INFO] epoch=11200, loss=0.0005283\n",
      "[INFO] epoch=11300, loss=0.0005226\n",
      "[INFO] epoch=11400, loss=0.0005170\n",
      "[INFO] epoch=11500, loss=0.0005115\n",
      "[INFO] epoch=11600, loss=0.0005062\n",
      "[INFO] epoch=11700, loss=0.0005009\n",
      "[INFO] epoch=11800, loss=0.0004957\n",
      "[INFO] epoch=11900, loss=0.0004907\n",
      "[INFO] epoch=12000, loss=0.0004857\n",
      "[INFO] epoch=12100, loss=0.0004809\n",
      "[INFO] epoch=12200, loss=0.0004761\n",
      "[INFO] epoch=12300, loss=0.0004714\n",
      "[INFO] epoch=12400, loss=0.0004669\n",
      "[INFO] epoch=12500, loss=0.0004624\n",
      "[INFO] epoch=12600, loss=0.0004580\n",
      "[INFO] epoch=12700, loss=0.0004536\n",
      "[INFO] epoch=12800, loss=0.0004494\n",
      "[INFO] epoch=12900, loss=0.0004452\n",
      "[INFO] epoch=13000, loss=0.0004411\n",
      "[INFO] epoch=13100, loss=0.0004371\n",
      "[INFO] epoch=13200, loss=0.0004331\n",
      "[INFO] epoch=13300, loss=0.0004293\n",
      "[INFO] epoch=13400, loss=0.0004255\n",
      "[INFO] epoch=13500, loss=0.0004217\n",
      "[INFO] epoch=13600, loss=0.0004180\n",
      "[INFO] epoch=13700, loss=0.0004144\n",
      "[INFO] epoch=13800, loss=0.0004108\n",
      "[INFO] epoch=13900, loss=0.0004073\n",
      "[INFO] epoch=14000, loss=0.0004039\n",
      "[INFO] epoch=14100, loss=0.0004005\n",
      "[INFO] epoch=14200, loss=0.0003972\n",
      "[INFO] epoch=14300, loss=0.0003939\n",
      "[INFO] epoch=14400, loss=0.0003907\n",
      "[INFO] epoch=14500, loss=0.0003875\n",
      "[INFO] epoch=14600, loss=0.0003844\n",
      "[INFO] epoch=14700, loss=0.0003813\n",
      "[INFO] epoch=14800, loss=0.0003783\n",
      "[INFO] epoch=14900, loss=0.0003753\n",
      "[INFO] epoch=15000, loss=0.0003724\n",
      "[INFO] epoch=15100, loss=0.0003695\n",
      "[INFO] epoch=15200, loss=0.0003667\n",
      "[INFO] epoch=15300, loss=0.0003639\n",
      "[INFO] epoch=15400, loss=0.0003611\n",
      "[INFO] epoch=15500, loss=0.0003584\n",
      "[INFO] epoch=15600, loss=0.0003557\n",
      "[INFO] epoch=15700, loss=0.0003531\n",
      "[INFO] epoch=15800, loss=0.0003505\n",
      "[INFO] epoch=15900, loss=0.0003479\n",
      "[INFO] epoch=16000, loss=0.0003454\n",
      "[INFO] epoch=16100, loss=0.0003429\n",
      "[INFO] epoch=16200, loss=0.0003404\n",
      "[INFO] epoch=16300, loss=0.0003380\n",
      "[INFO] epoch=16400, loss=0.0003356\n",
      "[INFO] epoch=16500, loss=0.0003333\n",
      "[INFO] epoch=16600, loss=0.0003309\n",
      "[INFO] epoch=16700, loss=0.0003286\n",
      "[INFO] epoch=16800, loss=0.0003264\n",
      "[INFO] epoch=16900, loss=0.0003242\n",
      "[INFO] epoch=17000, loss=0.0003220\n",
      "[INFO] epoch=17100, loss=0.0003198\n",
      "[INFO] epoch=17200, loss=0.0003176\n",
      "[INFO] epoch=17300, loss=0.0003155\n",
      "[INFO] epoch=17400, loss=0.0003134\n",
      "[INFO] epoch=17500, loss=0.0003114\n",
      "[INFO] epoch=17600, loss=0.0003094\n",
      "[INFO] epoch=17700, loss=0.0003073\n",
      "[INFO] epoch=17800, loss=0.0003054\n",
      "[INFO] epoch=17900, loss=0.0003034\n",
      "[INFO] epoch=18000, loss=0.0003015\n",
      "[INFO] epoch=18100, loss=0.0002996\n",
      "[INFO] epoch=18200, loss=0.0002977\n",
      "[INFO] epoch=18300, loss=0.0002958\n",
      "[INFO] epoch=18400, loss=0.0002940\n",
      "[INFO] epoch=18500, loss=0.0002922\n",
      "[INFO] epoch=18600, loss=0.0002904\n",
      "[INFO] epoch=18700, loss=0.0002886\n",
      "[INFO] epoch=18800, loss=0.0002869\n",
      "[INFO] epoch=18900, loss=0.0002851\n",
      "[INFO] epoch=19000, loss=0.0002834\n",
      "[INFO] epoch=19100, loss=0.0002817\n",
      "[INFO] epoch=19200, loss=0.0002801\n",
      "[INFO] epoch=19300, loss=0.0002784\n",
      "[INFO] epoch=19400, loss=0.0002768\n",
      "[INFO] epoch=19500, loss=0.0002752\n",
      "[INFO] epoch=19600, loss=0.0002736\n",
      "[INFO] epoch=19700, loss=0.0002720\n",
      "[INFO] epoch=19800, loss=0.0002704\n",
      "[INFO] epoch=19900, loss=0.0002689\n",
      "[INFO] epoch=20000, loss=0.0002674\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([2, 2, 1], alpha=0.5)\n",
    "nn.fit(X, y, epochs=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data=[0 0], ground-truth=0, pred=0.0087, step=0\n",
      "[INFO] data=[0 1], ground-truth=1, pred=0.9866, step=1\n",
      "[INFO] data=[1 0], ground-truth=1, pred=0.9893, step=1\n",
      "[INFO] data=[1 1], ground-truth=0, pred=0.0129, step=0\n"
     ]
    }
   ],
   "source": [
    "for (x, target) in zip(X, y):\n",
    "\t# make a prediction on the data point and display the result\n",
    "\t# to our console\n",
    "\tpred = nn.predict(x)[0][0]\n",
    "\tstep = 1 if pred > 0.5 else 0\n",
    "\tprint(\"[INFO] data={}, ground-truth={}, pred={:.4f}, step={}\".format(\n",
    "\t\tx, target[0], pred, step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch=1, loss=0.5274173\n",
      "[INFO] epoch=100, loss=0.5010323\n",
      "[INFO] epoch=200, loss=0.5007965\n",
      "[INFO] epoch=300, loss=0.5007934\n",
      "[INFO] epoch=400, loss=0.5007937\n",
      "[INFO] epoch=500, loss=0.5007938\n",
      "[INFO] epoch=600, loss=0.5007938\n",
      "[INFO] epoch=700, loss=0.5007938\n",
      "[INFO] epoch=800, loss=0.5007938\n",
      "[INFO] epoch=900, loss=0.5007938\n",
      "[INFO] epoch=1000, loss=0.5007938\n",
      "[INFO] epoch=1100, loss=0.5007938\n",
      "[INFO] epoch=1200, loss=0.5007938\n",
      "[INFO] epoch=1300, loss=0.5007938\n",
      "[INFO] epoch=1400, loss=0.5007938\n",
      "[INFO] epoch=1500, loss=0.5007938\n",
      "[INFO] epoch=1600, loss=0.5007938\n",
      "[INFO] epoch=1700, loss=0.5007938\n",
      "[INFO] epoch=1800, loss=0.5007938\n",
      "[INFO] epoch=1900, loss=0.5007938\n",
      "[INFO] epoch=2000, loss=0.5007938\n",
      "[INFO] epoch=2100, loss=0.5007938\n",
      "[INFO] epoch=2200, loss=0.5007938\n",
      "[INFO] epoch=2300, loss=0.5007938\n",
      "[INFO] epoch=2400, loss=0.5007938\n",
      "[INFO] epoch=2500, loss=0.5007938\n",
      "[INFO] epoch=2600, loss=0.5007938\n",
      "[INFO] epoch=2700, loss=0.5007938\n",
      "[INFO] epoch=2800, loss=0.5007938\n",
      "[INFO] epoch=2900, loss=0.5007938\n",
      "[INFO] epoch=3000, loss=0.5007938\n",
      "[INFO] epoch=3100, loss=0.5007938\n",
      "[INFO] epoch=3200, loss=0.5007938\n",
      "[INFO] epoch=3300, loss=0.5007938\n",
      "[INFO] epoch=3400, loss=0.5007938\n",
      "[INFO] epoch=3500, loss=0.5007938\n",
      "[INFO] epoch=3600, loss=0.5007938\n",
      "[INFO] epoch=3700, loss=0.5007938\n",
      "[INFO] epoch=3800, loss=0.5007938\n",
      "[INFO] epoch=3900, loss=0.5007938\n",
      "[INFO] epoch=4000, loss=0.5007938\n",
      "[INFO] epoch=4100, loss=0.5007938\n",
      "[INFO] epoch=4200, loss=0.5007938\n",
      "[INFO] epoch=4300, loss=0.5007938\n",
      "[INFO] epoch=4400, loss=0.5007938\n",
      "[INFO] epoch=4500, loss=0.5007938\n",
      "[INFO] epoch=4600, loss=0.5007938\n",
      "[INFO] epoch=4700, loss=0.5007938\n",
      "[INFO] epoch=4800, loss=0.5007938\n",
      "[INFO] epoch=4900, loss=0.5007938\n",
      "[INFO] epoch=5000, loss=0.5007938\n",
      "[INFO] epoch=5100, loss=0.5007938\n",
      "[INFO] epoch=5200, loss=0.5007938\n",
      "[INFO] epoch=5300, loss=0.5007938\n",
      "[INFO] epoch=5400, loss=0.5007938\n",
      "[INFO] epoch=5500, loss=0.5007938\n",
      "[INFO] epoch=5600, loss=0.5007938\n",
      "[INFO] epoch=5700, loss=0.5007938\n",
      "[INFO] epoch=5800, loss=0.5007938\n",
      "[INFO] epoch=5900, loss=0.5007938\n",
      "[INFO] epoch=6000, loss=0.5007938\n",
      "[INFO] epoch=6100, loss=0.5007938\n",
      "[INFO] epoch=6200, loss=0.5007938\n",
      "[INFO] epoch=6300, loss=0.5007938\n",
      "[INFO] epoch=6400, loss=0.5007938\n",
      "[INFO] epoch=6500, loss=0.5007938\n",
      "[INFO] epoch=6600, loss=0.5007938\n",
      "[INFO] epoch=6700, loss=0.5007938\n",
      "[INFO] epoch=6800, loss=0.5007938\n",
      "[INFO] epoch=6900, loss=0.5007938\n",
      "[INFO] epoch=7000, loss=0.5007938\n",
      "[INFO] epoch=7100, loss=0.5007938\n",
      "[INFO] epoch=7200, loss=0.5007938\n",
      "[INFO] epoch=7300, loss=0.5007938\n",
      "[INFO] epoch=7400, loss=0.5007938\n",
      "[INFO] epoch=7500, loss=0.5007938\n",
      "[INFO] epoch=7600, loss=0.5007938\n",
      "[INFO] epoch=7700, loss=0.5007938\n",
      "[INFO] epoch=7800, loss=0.5007938\n",
      "[INFO] epoch=7900, loss=0.5007938\n",
      "[INFO] epoch=8000, loss=0.5007938\n",
      "[INFO] epoch=8100, loss=0.5007938\n",
      "[INFO] epoch=8200, loss=0.5007938\n",
      "[INFO] epoch=8300, loss=0.5007938\n",
      "[INFO] epoch=8400, loss=0.5007938\n",
      "[INFO] epoch=8500, loss=0.5007938\n",
      "[INFO] epoch=8600, loss=0.5007938\n",
      "[INFO] epoch=8700, loss=0.5007938\n",
      "[INFO] epoch=8800, loss=0.5007938\n",
      "[INFO] epoch=8900, loss=0.5007938\n",
      "[INFO] epoch=9000, loss=0.5007938\n",
      "[INFO] epoch=9100, loss=0.5007938\n",
      "[INFO] epoch=9200, loss=0.5007938\n",
      "[INFO] epoch=9300, loss=0.5007938\n",
      "[INFO] epoch=9400, loss=0.5007938\n",
      "[INFO] epoch=9500, loss=0.5007938\n",
      "[INFO] epoch=9600, loss=0.5007938\n",
      "[INFO] epoch=9700, loss=0.5007938\n",
      "[INFO] epoch=9800, loss=0.5007938\n",
      "[INFO] epoch=9900, loss=0.5007938\n",
      "[INFO] epoch=10000, loss=0.5007938\n",
      "[INFO] epoch=10100, loss=0.5007938\n",
      "[INFO] epoch=10200, loss=0.5007938\n",
      "[INFO] epoch=10300, loss=0.5007938\n",
      "[INFO] epoch=10400, loss=0.5007938\n",
      "[INFO] epoch=10500, loss=0.5007938\n",
      "[INFO] epoch=10600, loss=0.5007938\n",
      "[INFO] epoch=10700, loss=0.5007938\n",
      "[INFO] epoch=10800, loss=0.5007938\n",
      "[INFO] epoch=10900, loss=0.5007938\n",
      "[INFO] epoch=11000, loss=0.5007938\n",
      "[INFO] epoch=11100, loss=0.5007938\n",
      "[INFO] epoch=11200, loss=0.5007938\n",
      "[INFO] epoch=11300, loss=0.5007938\n",
      "[INFO] epoch=11400, loss=0.5007938\n",
      "[INFO] epoch=11500, loss=0.5007938\n",
      "[INFO] epoch=11600, loss=0.5007938\n",
      "[INFO] epoch=11700, loss=0.5007938\n",
      "[INFO] epoch=11800, loss=0.5007938\n",
      "[INFO] epoch=11900, loss=0.5007938\n",
      "[INFO] epoch=12000, loss=0.5007938\n",
      "[INFO] epoch=12100, loss=0.5007938\n",
      "[INFO] epoch=12200, loss=0.5007938\n",
      "[INFO] epoch=12300, loss=0.5007938\n",
      "[INFO] epoch=12400, loss=0.5007938\n",
      "[INFO] epoch=12500, loss=0.5007938\n",
      "[INFO] epoch=12600, loss=0.5007938\n",
      "[INFO] epoch=12700, loss=0.5007938\n",
      "[INFO] epoch=12800, loss=0.5007938\n",
      "[INFO] epoch=12900, loss=0.5007938\n",
      "[INFO] epoch=13000, loss=0.5007938\n",
      "[INFO] epoch=13100, loss=0.5007938\n",
      "[INFO] epoch=13200, loss=0.5007938\n",
      "[INFO] epoch=13300, loss=0.5007938\n",
      "[INFO] epoch=13400, loss=0.5007938\n",
      "[INFO] epoch=13500, loss=0.5007938\n",
      "[INFO] epoch=13600, loss=0.5007938\n",
      "[INFO] epoch=13700, loss=0.5007938\n",
      "[INFO] epoch=13800, loss=0.5007938\n",
      "[INFO] epoch=13900, loss=0.5007938\n",
      "[INFO] epoch=14000, loss=0.5007938\n",
      "[INFO] epoch=14100, loss=0.5007938\n",
      "[INFO] epoch=14200, loss=0.5007938\n",
      "[INFO] epoch=14300, loss=0.5007938\n",
      "[INFO] epoch=14400, loss=0.5007938\n",
      "[INFO] epoch=14500, loss=0.5007938\n",
      "[INFO] epoch=14600, loss=0.5007938\n",
      "[INFO] epoch=14700, loss=0.5007938\n",
      "[INFO] epoch=14800, loss=0.5007938\n",
      "[INFO] epoch=14900, loss=0.5007938\n",
      "[INFO] epoch=15000, loss=0.5007938\n",
      "[INFO] epoch=15100, loss=0.5007938\n",
      "[INFO] epoch=15200, loss=0.5007938\n",
      "[INFO] epoch=15300, loss=0.5007938\n",
      "[INFO] epoch=15400, loss=0.5007938\n",
      "[INFO] epoch=15500, loss=0.5007938\n",
      "[INFO] epoch=15600, loss=0.5007938\n",
      "[INFO] epoch=15700, loss=0.5007938\n",
      "[INFO] epoch=15800, loss=0.5007938\n",
      "[INFO] epoch=15900, loss=0.5007938\n",
      "[INFO] epoch=16000, loss=0.5007938\n",
      "[INFO] epoch=16100, loss=0.5007938\n",
      "[INFO] epoch=16200, loss=0.5007938\n",
      "[INFO] epoch=16300, loss=0.5007938\n",
      "[INFO] epoch=16400, loss=0.5007938\n",
      "[INFO] epoch=16500, loss=0.5007938\n",
      "[INFO] epoch=16600, loss=0.5007938\n",
      "[INFO] epoch=16700, loss=0.5007938\n",
      "[INFO] epoch=16800, loss=0.5007938\n",
      "[INFO] epoch=16900, loss=0.5007938\n",
      "[INFO] epoch=17000, loss=0.5007938\n",
      "[INFO] epoch=17100, loss=0.5007938\n",
      "[INFO] epoch=17200, loss=0.5007938\n",
      "[INFO] epoch=17300, loss=0.5007938\n",
      "[INFO] epoch=17400, loss=0.5007938\n",
      "[INFO] epoch=17500, loss=0.5007938\n",
      "[INFO] epoch=17600, loss=0.5007938\n",
      "[INFO] epoch=17700, loss=0.5007938\n",
      "[INFO] epoch=17800, loss=0.5007938\n",
      "[INFO] epoch=17900, loss=0.5007938\n",
      "[INFO] epoch=18000, loss=0.5007938\n",
      "[INFO] epoch=18100, loss=0.5007938\n",
      "[INFO] epoch=18200, loss=0.5007938\n",
      "[INFO] epoch=18300, loss=0.5007938\n",
      "[INFO] epoch=18400, loss=0.5007938\n",
      "[INFO] epoch=18500, loss=0.5007938\n",
      "[INFO] epoch=18600, loss=0.5007938\n",
      "[INFO] epoch=18700, loss=0.5007938\n",
      "[INFO] epoch=18800, loss=0.5007938\n",
      "[INFO] epoch=18900, loss=0.5007938\n",
      "[INFO] epoch=19000, loss=0.5007938\n",
      "[INFO] epoch=19100, loss=0.5007938\n",
      "[INFO] epoch=19200, loss=0.5007938\n",
      "[INFO] epoch=19300, loss=0.5007938\n",
      "[INFO] epoch=19400, loss=0.5007938\n",
      "[INFO] epoch=19500, loss=0.5007938\n",
      "[INFO] epoch=19600, loss=0.5007938\n",
      "[INFO] epoch=19700, loss=0.5007938\n",
      "[INFO] epoch=19800, loss=0.5007938\n",
      "[INFO] epoch=19900, loss=0.5007938\n",
      "[INFO] epoch=20000, loss=0.5007938\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([2, 1], alpha=0.5)\n",
    "nn.fit(X, y, epochs=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data=[0 0], ground-truth=0, pred=0.5161, step=1\n",
      "[INFO] data=[0 1], ground-truth=1, pred=0.5000, step=0\n",
      "[INFO] data=[1 0], ground-truth=1, pred=0.4839, step=0\n",
      "[INFO] data=[1 1], ground-truth=0, pred=0.4678, step=0\n"
     ]
    }
   ],
   "source": [
    "for (x, target) in zip(X, y):\n",
    "\t# make a prediction on the data point and display the result\n",
    "\t# to our console\n",
    "\tpred = nn.predict(x)[0][0]\n",
    "\tstep = 1 if pred > 0.5 else 0\n",
    "\tprint(\"[INFO] data={}, ground-truth={}, pred={:.4f}, step={}\".format(\n",
    "\t\tx, target[0], pred, step))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
