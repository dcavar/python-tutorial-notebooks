{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn Logistic Regression\n",
    "\n",
    "(C) 2023-2024 by [Damir Cavar](http://damir.cavar.me/)\n",
    "\n",
    "**Version:** 1.2, September 2024\n",
    "\n",
    "**Download:** This and various other Jupyter notebooks are available from my [GitHub repo](https://github.com/dcavar/python-tutorial-notebooks).\n",
    "\n",
    "**License:** [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/) ([CA BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/))\n",
    "\n",
    "This tutorial was developed as part of the course material for the course Advanced Natural Language Processing at [Indiana University](https://www.indiana.edu/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOC\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Learning Weights](#learning-weights)\n",
    "- [Gradient Descent](#gradient-descent)\n",
    "- [Using Scikit-Learn](#using-scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example problems are taken from the textbook Dan Jurafsky and James H. Martin (2023 draft) [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) chapter 5 on Logistic Regression. The code is written by [Damir Cavar](http://damir.cavar.me/) and simplified for use in the Advanced Natural Language Processing course taught at Indiana University in Fall 2023 and 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code we import all the used modules. You will need to make sure that [Scikit-learn](https://scikit-learn.org/stable/), and [NLTK](https://www.nltk.org/) are installed. You will need to implement a sigmoid function in a file called *secret.py* in the local folder. Since this is part of an assignment, this is not shared here yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:21:18.830441Z",
     "start_time": "2025-09-24T15:21:17.783093Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "from secret import sigmoid\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import zipfile\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:21:20.235141Z",
     "start_time": "2025-09-24T15:21:20.232107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/damir/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vader Lexicon file can be found in the NLTK data in `nltk-data/sentiment/vader_lexicon.zip`. It contains a list of tokens with sentiment ratings. Each line represents one token and the tab-seperated values are:\n",
    "- token\n",
    "- the mean of the human sentiment ratings\n",
    "- the Standard Deviation of the token\n",
    "- the list of 10 human ratings taken during experiments\n",
    "\n",
    "In the following the assumption is that the Vader lexicon is located in your `nltk-data`-folder. On Linux systems this is per default in your home directory. On Windows this is in your `AppData\\roaming` folder.\n",
    "\n",
    "We can read the Vader lexicon from the NLTK zip-file into a dictionary structure as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:21:41.261142Z",
     "start_time": "2025-09-24T15:21:41.125908Z"
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "username = \"damir\"\n",
    "# On Linux:\n",
    "nltk_data_folder = f\"/home/{username}/nltk_data\"\n",
    "# On Windows:\n",
    "# nltk_data_folder = f\"C:/Users/{username}/AppData/Roaming/nltk_data\"\n",
    "vader_filename = \"vader_lexicon/vader_lexicon.txt\"\n",
    "vader_data = {}\n",
    "with zipfile.ZipFile(os.path.join(nltk_data_folder, \"sentiment\", 'vader_lexicon.zip')) as z:\n",
    "    if vader_filename in z.namelist():\n",
    "        with z.open(vader_filename) as f:\n",
    "            for l in f:\n",
    "                tokens = l.decode(encoding='utf-8').strip().split('\\t')\n",
    "                if len(tokens) != 4:\n",
    "                    continue\n",
    "                vader_data[tokens[0]] = (float(tokens[1]), float(tokens[2]), ast.literal_eval(tokens[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the lexicon is in the data folder, as it should be, the code below will read in the data into the dictionary `vader_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_filename = \"data/vader_lexicon.txt\"\n",
    "vader_data = {}\n",
    "if os.path.exists(vader_filename):\n",
    "\twith open(vader_filename, 'r', encoding='utf-8') as f:\n",
    "\t\tfor l in f:\n",
    "\t\t\ttokens = l.strip().split('\\t')\n",
    "\t\t\tif len(tokens) != 4:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tvader_data[tokens[0]] = (float(tokens[1]), float(tokens[2]), ast.literal_eval(tokens[3]))\n",
    "else:\n",
    "\tprint(f\"File {vader_filename} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now request the scores for existing tokens from the `vadar_data` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:21:52.850989Z",
     "start_time": "2025-09-24T15:21:52.848010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.6, 0.66332, [2, 3, 3, 3, 4, 3, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(vader_data[\"admirable\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assume that positive scores indicate that the token is typical for positive sentiment, while negative scores represent negative sentiment. We can see that for example when pulling the scores for token `annoying`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:21:55.436508Z",
     "start_time": "2025-09-24T15:21:55.431989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1.7, 0.64031, [-1, -2, -1, -2, -1, -1, -2, -2, -3, -2])\n"
     ]
    }
   ],
   "source": [
    "print(vader_data[\"annoying\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the textbook the feature vector is generated using the following scores:\n",
    "- number of positive terms in text\n",
    "- number of negative terms\n",
    "- 1, if there is a *no* in the text, 0 if there is none\n",
    "- number of pronouns, all variants of 1st and 2nd person\n",
    "- 1 if there is a *!* in the text, 0 if there is none\n",
    "- the log of the number of tokens\n",
    "\n",
    "The following function generates a feature vector from some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:22:00.978786Z",
     "start_time": "2025-09-24T15:22:00.974214Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_feature_vector(text: str) -> list:\n",
    "    tokens = word_tokenize(text)\n",
    "    scores = [ vader_data.get(t, [0, 0]) for t in tokens ]\n",
    "    negative_terms = sum(1 for i in scores if i[0] < 0)\n",
    "    positive_terms = sum(1 for i in scores if i[0] > 0)\n",
    "    if \"no\" in tokens:\n",
    "        no_in_text = 1\n",
    "    else:\n",
    "        no_in_text = 0\n",
    "    pronouns = set( (\"I\", \"you\", \"me\", \"your\", \"mine\") )\n",
    "    count_pronouns = sum(1 for i in tokens if i in pronouns)\n",
    "    if \"!\" in tokens:\n",
    "        excl_in_text = 1\n",
    "    else:\n",
    "        excl_in_text = 0\n",
    "    return np.array([positive_terms, negative_terms, no_in_text, count_pronouns, excl_in_text, math.log(len(tokens))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use some sample text and generate a feature vector for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:22:05.921876Z",
     "start_time": "2025-09-24T15:22:05.919311Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_text = \"\"\"It's hokey. There are virtually no surprises, and the writing is second-rate.\n",
    "So why was it so enjoyable? For one thing, the cast is great.\n",
    "Another nice touch is the music.\n",
    "I was overcome with the urge to get off the couch and start dancing.\n",
    "It sucked me in, and it'll do the same to you.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature vector is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:22:14.888578Z",
     "start_time": "2025-09-24T15:22:14.861648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.         2.         1.         3.         0.         4.21950771]\n"
     ]
    }
   ],
   "source": [
    "sample_text_vector = generate_feature_vector(sample_text)\n",
    "print(sample_text_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The textbook approach uses different vocabulary and entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:22:19.293594Z",
     "start_time": "2025-09-24T15:22:19.289003Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_text_vector_textbook = np.array([3, 2, 1, 3, 0, 4.19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:22:24.018466Z",
     "start_time": "2025-09-24T15:22:24.012458Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = np.array([2.5, -5.0, -1.2, 0.5, 2.0, 0.7])\n",
    "b = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the sigmoid scores for the feature vectors generated from the Vader lexicon is given in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:22:26.947310Z",
     "start_time": "2025-09-24T15:22:26.938656Z"
    }
   },
   "outputs": [],
   "source": [
    "sigmoid_positive = sigmoid( np.dot(weights, sample_text_vector) + b )\n",
    "sigmoid_negative = 1 - sigmoid_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is classified as `positive sentiment` with 96% likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:22:29.801539Z",
     "start_time": "2025-09-24T15:22:29.798521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9662243326599138 0.03377566734008619\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid_positive, sigmoid_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the textbook scores for the text vector, our sigmoid values are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:22:39.040361Z",
     "start_time": "2025-09-24T15:22:39.037617Z"
    }
   },
   "outputs": [],
   "source": [
    "sigmoid_positive = sigmoid( np.dot(weights, sample_text_vector_textbook) + b )\n",
    "sigmoid_negative = 1 - sigmoid_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is still judged as `positive sentiment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:22:44.011828Z",
     "start_time": "2025-09-24T15:22:44.009263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6969888901292717 0.3030111098707283\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid_positive, sigmoid_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Weights <a class=\"anchor\" id=\"learning-weights\"></a>\n",
    "\n",
    "The weights in the previos section have been manually set. In the following we will go over a strategy to learn those weights using the Cross-entropy Loss Function and Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the problem discussed in the textbook, the Cross-entropy Loss Function is defined as:\n",
    "\n",
    "$L_{CE}(\\hat{y},y) = -\\, log\\, p(y|x) = -\\, [y\\, log(\\hat{y}) + (1-y)\\, log(1-\\hat{y})] $\n",
    "\n",
    "If `y=1`, the second summand in the equation become `0`, thus we only look at:\n",
    "\n",
    "$L_{CE}(\\hat{y},1) = -\\, log\\, p(1|x) = -\\, 1\\, log(\\hat{y}) = -\\, log(\\hat{y})$\n",
    "\n",
    "For `y=0`, the first summand in the equation becomes `0`, this we only look at:\n",
    "\n",
    "$L_{CE}(\\hat{y},0) = -\\, log\\, p(0|x) = -\\, (1-0)\\, log(1-\\hat{y}) = -\\, log(1-\\hat{y}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:22:50.962984Z",
     "start_time": "2025-09-24T15:22:50.961028Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_hat, y):\n",
    "  return -np.log(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y}$ is the `sigmoid` of the dot-product of the weight and feature vector after adding the bias value to it. That is, we can compute the `cross-entropy` from the `sigmoid` scores above\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:22:54.069021Z",
     "start_time": "2025-09-24T15:22:54.066509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid 0.6969888901292717 for y=1 Loss positive: 0.3609858079049309\n"
     ]
    }
   ],
   "source": [
    "cel_positive = cross_entropy_loss(sigmoid_positive, 1)\n",
    "print(f\"sigmoid {sigmoid_positive} for y={1} Loss positive: {cel_positive}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:22:56.805389Z",
     "start_time": "2025-09-24T15:22:56.803333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid 0.3030111098707283 for y=0 Loss positive: 1.1939858079049306\n"
     ]
    }
   ],
   "source": [
    "cel_negative = cross_entropy_loss(sigmoid_negative, 0)\n",
    "print(f\"sigmoid {sigmoid_negative} for y={0} Loss positive: {cel_negative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent <a class=\"anchor\" id=\"gradient-descent\"></a>\n",
    "\n",
    "Loss function is paretrized by the weights $\\theta = (w,b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:23:04.125545Z",
     "start_time": "2025-09-24T15:23:04.121467Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"\"\"It's hokey. There are virtually no surprises, and the writing is second-rate.\n",
    "So why was it so enjoyable? For one thing, the cast is great.\n",
    "Another nice touch is the music.\n",
    "I was overcome with the urge to get off the couch and start dancing.\n",
    "It sucked me in, and it'll do the same to you.\"\"\", 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stochastic gradient descent function in our simple classification task can be defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:23:05.730484Z",
     "start_time": "2025-09-24T15:23:05.727570Z"
    }
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(data):\n",
    "    w = np.array([0, 0, 0, 0, 0, 0])\n",
    "    b = 0\n",
    "    learning_rate = 0.1\n",
    "    for text, y in data:\n",
    "        x = generate_feature_vector(text)\n",
    "        print(\"x:\", x)\n",
    "        y_hat = sigmoid( np.dot(w, x) + b )\n",
    "        print(\"y_hat:\", y_hat)\n",
    "        gradient_b = y_hat - y\n",
    "        print(\"gradient b:\", gradient_b)\n",
    "        b = b - learning_rate * gradient_b\n",
    "        print(\"new gradient b:\", b)\n",
    "        gradient_w = (y_hat - y) * x\n",
    "        print(\"gradient w:\", gradient_w)\n",
    "        # w = gradient_w - learning_rate * gradient_w\n",
    "        w = w - learning_rate * gradient_w\n",
    "        print(\"new weights w:\", w)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:23:10.622441Z",
     "start_time": "2025-09-24T15:23:10.619257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [4.         2.         1.         3.         0.         4.21950771]\n",
      "y_hat: 0.5\n",
      "gradient b: -0.5\n",
      "new gradient b: 0.05\n",
      "gradient w: [-2.         -1.         -0.5        -1.5        -0.         -2.10975385]\n",
      "new weights w: [0.2        0.1        0.05       0.15       0.         0.21097539]\n"
     ]
    }
   ],
   "source": [
    "w, b = stochastic_gradient_descent(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:23:28.604696Z",
     "start_time": "2025-09-24T15:23:28.601495Z"
    }
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_silent(data):\n",
    "    w = np.array([0, 0, 0, 0, 0, 0])\n",
    "    b = 0\n",
    "    learning_rate = 0.1\n",
    "    for text, y in data:\n",
    "        x = generate_feature_vector(text)\n",
    "        y_hat = sigmoid( np.dot(w, x) + b )\n",
    "        gradient_b = y_hat - y\n",
    "        b = b - learning_rate * gradient_b\n",
    "        gradient_w = (y_hat - y) * x\n",
    "        w = w - learning_rate * gradient_w\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:23:33.581591Z",
     "start_time": "2025-09-24T15:23:33.579772Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:23:36.023478Z",
     "start_time": "2025-09-24T15:23:35.644497Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join('.', 'data', 'reviews.csv'), newline='') as csvfile:\n",
    "    datareader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    header = next(datareader)\n",
    "    for row in datareader:\n",
    "        if len(row) == 2:\n",
    "            experiment_data.append( [row[0].strip(), int(row[1].strip())] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:23:38.516725Z",
     "start_time": "2025-09-24T15:23:38.507018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 25000\t Negative: 25000\n",
      "Total reviews: 50000\n"
     ]
    }
   ],
   "source": [
    "count_positive = sum([ 1 for x in experiment_data if x[1] == 1 ])\n",
    "count_negative = sum([ 1 for x in experiment_data if x[1] == 0 ])\n",
    "print(f\"Positive: {count_positive}\\t Negative: {count_negative}\")\n",
    "print(\"Total reviews:\", len(experiment_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:23:43.402931Z",
     "start_time": "2025-09-24T15:23:43.398999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\", 0], [\"This is an example of why the majority of action films are the same. Generic and boring, there's really nothing worth watching here. A complete waste of the then barely-tapped talents of Ice-T and Ice Cube, who've each proven many times over that they are capable of acting, and acting well. Don't bother with this one, go see New Jack City, Ricochet or watch New York Undercover for Ice-T, or Boyz n the Hood, Higher Learning or Friday for Ice Cube and see the real deal. Ice-T's horribly cliched dialogue alone makes this film grate at the teeth, and I'm still wondering what the heck Bill Paxton was doing in this film? And why the heck does he always play the exact same character? From Aliens onward, every film I've seen with Bill Paxton has him playing the exact same irritating character, and at least in Aliens his character died, which made it somewhat gratifying... Overall, this is second-rate action trash. There are countless better films to see, and if you really want to see this one, watch Judgement Night, which is practically a carbon copy but has better acting and a better script. The only thing that made this at all worth watching was a decent hand on the camera - the cinematography was almost refreshing, which comes close to making up for the horrible film itself - but not quite. 4/10.\", 0], [\"First of all I hate those moronic rappers, who could'nt act if they had a gun pressed against their foreheads. All they do is curse and shoot each other and acting like cliché'e version of gangsters. The movie doesn't take more than five minutes to explain what is going on before we're already at the warehouse There is not a single sympathetic character in this movie, except for the homeless guy, who is also the only one with half a brain. Bill Paxton and William Sadler are both hill billies and Sadlers character is just as much a villain as the gangsters. I did'nt like him right from the start. The movie is filled with pointless violence and Walter Hills specialty: people falling through windows with glass flying everywhere. There is pretty much no plot and it is a big problem when you root for no-one. Everybody dies, except from Paxton and the homeless guy and everybody get what they deserve. The only two black people that can act is the homeless guy and the junkie but they're actors by profession, not annoying ugly brain dead rappers. Stay away from this crap and watch 48 hours 1 and 2 instead. At lest they have characters you care about, a sense of humor and nothing but real actors in the cast.\", 0]]\n",
      "[0.2        0.1        0.05       0.15       0.         0.21097539] 0.05\n"
     ]
    }
   ],
   "source": [
    "print(experiment_data[:3])\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:23:48.023224Z",
     "start_time": "2025-09-24T15:23:47.986629Z"
    }
   },
   "outputs": [],
   "source": [
    "w, b = stochastic_gradient_descent_silent(experiment_data[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = stochastic_gradient_descent_silent(experiment_data) # [:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:23:51.061904Z",
     "start_time": "2025-09-24T15:23:51.058596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.6508892   0.49448536 -0.0074577   0.84723691 -0.00302566  0.96923873] 0.206158278233459\n"
     ]
    }
   ],
   "source": [
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:23:55.185498Z",
     "start_time": "2025-09-24T15:23:55.181854Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(data, w, b):\n",
    "    res = []\n",
    "    for text, y in data:\n",
    "        x = generate_feature_vector(text)\n",
    "        y_hat = sigmoid( np.dot(w, x) + b )\n",
    "        if y_hat > .5:\n",
    "            y_hat = 1\n",
    "        else:\n",
    "            y_hat = 0\n",
    "        res.append( (y, y_hat) )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:23:57.380511Z",
     "start_time": "2025-09-24T15:23:57.367891Z"
    }
   },
   "outputs": [],
   "source": [
    "#result = test(experiment_data[-20:], w, b)\n",
    "result = test(experiment_data, w, b) #-200:], w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:23:59.275097Z",
     "start_time": "2025-09-24T15:23:59.273385Z"
    }
   },
   "outputs": [],
   "source": [
    "counts = Counter(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:24:00.996182Z",
     "start_time": "2025-09-24T15:24:00.993961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(0, 1): 25000, (1, 1): 25000})\n"
     ]
    }
   ],
   "source": [
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn <a class=\"anchor\" id=\"using-scikit-learn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the data from the reviews data set above. For that we will create two lists containing the text and the label respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:24:08.824836Z",
     "start_time": "2025-09-24T15:24:08.813717Z"
    }
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "data_labels = []\n",
    "for e in experiment_data:\n",
    "    data.append(e[0])\n",
    "    if e[1] == 1:\n",
    "        data_labels.append('pos')\n",
    "    else:\n",
    "        data_labels.append('neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be transformed into feature vectors that take frequency into account, remove function words, and so on. The tokens or text is not normalized to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:24:22.884283Z",
     "start_time": "2025-09-24T15:24:12.116401Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = 'word', lowercase = False,)\n",
    "features = vectorizer.fit_transform(data)\n",
    "features_nd = features.toarray() # for easy usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the corpus into a training and test corpus, using 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:27:05.259116Z",
     "start_time": "2025-09-24T15:24:22.890853Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        features_nd, \n",
    "        data_labels,\n",
    "        train_size=0.80, \n",
    "        random_state=1234)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Logistic Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:40:28.911705Z",
     "start_time": "2025-09-24T15:40:28.902850Z"
    }
   },
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T15:56:09.187247Z",
     "start_time": "2025-09-24T15:40:31.102524Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "log_model = log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the prediction on the 20% test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T16:21:14.332899Z",
     "start_time": "2025-09-24T16:21:14.260676Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mlog_model\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log_model' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the results for a few random examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "Creakiness and atmosphere this film has, but so unfortunately does the print I just viewed. Raymond Massey provides a laid back Sherlock Holmes, almost comically so in early scenes in his bathrobe, which he trades in for a laborer's garb to investigate the creepy mansion of Dr. Rylott (Lyn Harding). What wasn't clear to me was why Rylott would have wanted his stepdaughters dead. If as in the case of Helen (Angela Baddeley), he didn't want her to run off to get married, he would have accomplished the same thing by having her dispatched. Other curiosities abound as well. After setting an early wedding date with Helen, the fiancÃ©e is no longer heard from for the rest of the picture. The presence of a band of gypsies at the time of Violet Stoner's death provides merely a diversion, and what could have been an interesting murder tool, a poisonous snake, is diluted by the fact that it was not a cobra, the musical renderings of the Indian man servant notwithstanding. Athole Stewart competently portrays Holmes' aide Dr. Watson, though he takes some getting used to if Nigel Bruce is more your cup of tea. As Rylott, Lyn Harding is sufficiently menacing, a trait that would be put to good use as Holmes' nemesis Professor Moriarty in two later films - 1935's \"The Triumph of Sherlock Holmes\" and 1937's \"Murder at the Baskervilles\". With repeated choppiness and an unsteady camera, it's surprising that the story line isn't more disrupted than it is. It's integrity is generally maintained, even if one stretches a bit to fill in the gaps. I guess that would be my main complaint with the film, as mentally bridging some of the jump cuts in the picture proved to be a real pain in the asp.\n",
      "pos\n",
      "As a father of four in his forties I thought this film made compelling viewing - if not edge-of-the-seat stuff. I deserves a far higher rating than the 4.3 that it had when I wrote this. (I gave it 7.) I agree with some of the comments about the characters but Cameron Diaz was, again, sparkling in yet another very different role. The plot was a little silly but the point of the film for me was beautifully summed up in the final, quite surreal, sequence. A moving ending for any parent. I could imagine that a young, single bloke might find the film quite boring but for other people not fixed on high doses of testosterone would find something sweet in this.\n",
      "pos\n",
      "As soon as I knew Keira Knighteley being in this flick, I said \"I have to watch this movie\". She's the undisputed main character, Domino, a bounty hunter. Her \"job\", as the \"no action\" scenes would teach us, reflects her rebel, violent attitude to life. I have to admit that it's the very first time that I watch an action movie whose most important scenes are the one in which the guns are far away from characters' hands. So, this stomped me a bit. Anyway, for all the John Woo's fans, there are helicopters falling down, explosions, gunfire as if it would rain, and a lovely Keira that shoots with two machine guns, one per hand. The cast is absolutely brilliant. Going beyond Keira, which in this movie is a real tomboy, pretty much different from the lovely action figure we're used to see, Mickey Rourke is back, with his usual slap-throwing face and his potent body. Christopher Walken makes his job pretty well, as a reality show producer. Let's go to the contents: this movie has a journalistic shape. The talk show scene is \"disgustingly\" real. Anyone that watched that load of . . . you-know-what, can tell that this is the air that you breath in those situations. As well as the producer, when Domino's mom says that the reality that should show Domino's life is \"trash; no offense\", he answers \"I don't take it like an offense\". This movie portrays a difficult life. Domino, coming from a world that didn't want her, Ed (Mickey Rourke), a bounty hunter \"not so bounty\", Choco (the third guy of the band), which family is (using Ed's words), \"the correctional institutes he's been\", and Alf, the driver/bomber coming from Afghanistan during the Russian occupation. This bunch of people represents in some way the humankind born \"without the shirt\"; unlucky, violent, and with nothing to lose, excepts their (as they would consider) miserable lives. The intro of the movie says that it is \"inspired by true story . . . more or less\", so I couldn't possibly tell you how much of this stuff is true. Anyway, going beyond explosions and dozens of weapons (which could have been \"added\" to make the film easier to see, and be classified as an action movie), the characters' story is too realistic to be \"edulcorated\".  The interaction between the characters is various, well studied, and definitely not boring. What hasn't convinced me so much is the role of the psychiatrist (Lucy Liu, sober as never in her acting career). It represents only the reason by which Domino starts telling her story (and that's a story). Probably, the only \"con\", in a movie with a lot of \"pros\".  All in all: This isn't \"SWAT\". The characters are crafted; they have an identity, a shape. They have a name and a surname (not just \"Gamble\" and \"Street\"). It's the case to say, it's the biography of a girl whose life went as fast as a bullet.\n",
      "pos\n",
      "Gregory Peck gives a brilliant performance in this film. The last 15 minutes (or thereabouts) are great and Peck is an absolute joy to watch. The same cannot however be said for the rest of the film. It's not awful and I'm sure it was made with good intentions, but the only real reason (if I were to be honest) to see it is Peck. For the rest you are better off just reading the Old Testament.\n",
      "pos\n",
      "All in all, don't expect much and you won't be disappointed. And if you want to see a movie that will take you back to 1983, this will do that for sure. The only reason I gave this movie 2 points more than it deserves is for 2 reasons: #1. Michael Caine #2. the people, the sights, the culture and the music of Brazil  The movie is almost completely carried by Caine as he commits the seemingly impossible task of transforming it into a viable and semi-believable story. Even Joe Bologna and Valerie Harper fall short.  Michael Caine is pure class, as always. Besides being a gifted classical and comic actor, Caine brings a blend of introspection, mischievousness and sensitivity to every movie he does ... the focus of his charm as far back as his role in Alfie...and the reason why he won the Academy Award for Hannah and her Sisters 2 years later. In this farce, he is tenderly beguiling...funny and vulnerable... melancholy and sentimental....and besides the jewel that is Rio de Janeiro, the ONLY reason to not seek out a better form of entertainment. Well...maybe a glimpse at the 2 lovely young actresses, Michelle Johnson and Demi Moore would be a reason. But look is all you can do at Michelle (though her look seems sorely dated)....there couldn't be a more painful movie experience than watching her \"try\" to act (most of her dialogue seems overdubbed, too). Demi's acting and looks hold up 100 times better and you could easily transplant her, as is, into any movie today (she doesn't really look much different to be honest). Ms. Moore is surely underused, especially considering she was the bigger star of the 2.  Save the fact that it is a silly farce, at the end, I actually kind of like the maturity with which all these people handle this scandalous situation...that it doesn't end friendships nor marriages and that an affair, even with the underage daughter of your best friend, could be forgiven and everyone can move on. The injured parties do show anger and disappointment at what transpired, but all works out for the best....a bit unrealistic for sure, but surprisingly refreshing. Hope always is.\n",
      "pos\n",
      "This is one of those star-filled over-the-top comedies that could a) be hysterical, or b) wish that you had gone to the dentist to have all your teeth pulled instead. Unfortunately, One Night at McCool's is a classic \"b.\" Goldie Hawn recently commented about \"Town and Country\" that it's a big problem in Hollywood that they start with hiring the actors and putting together a deal before a script is completed. You have to figure that not only did they go into this picture without a complete script, they also mangled it daily. Maybe we need to send cards and letters to the heads of all the studio that say, \"It's the script, stupid.\"  This is also one of those movies where you find yourself feeling sorry for the actors most of the way through. They're working their asses off trying to make all this seem hysterical, but they know most of it is going to be accompanied not by belly laughs but by the sounds of the crickets you can hear inside the silent theatre. Is it an unmitigated disaster? Not entirely. There are some smiles along the way, mostly due to the efforts of the actors. I probably would have gone out of the theatre thinking, \"Eh. It was okay.\" So why the undeniably hostile tone in my review? The ending. If, as it's been noted, the rest of the movie is just all a setup for the ending, then it misses spectacularly. I really wish I could speak specifically about it, but I hate people who give too much away (even in warning). Suffice it to say that as soon as you see John Goodman behind a bent-over Paul Reiser (nothing given away here. It's in the trailer), get the hell out of the theatre and go out thinking, \"Eh. It was okay.\" The rest of the movie is tacked-on and creatively bankrupt. And you'll be appalled that there will actually be people laughing at this mess.  If you loved \"There's Something About Mary\" or \"Meet The Parents\" (both GREAT movies), then don't bother to see this movie. Go have those teeth taken care of instead.\n",
      "pos\n",
      "from the view of a NASCAR Maniac like I am, the movie is interesting. You can see many race cars from 1983. Even tough, the racing scenes are not that much realistic. But I have to admit, that I haven't seen any race before 1995, because before that time, they didn't show any NASCAR races in Germany) from the view of a Burt Reynolds fan like I am, the movie basically is what we are used to see from Reynolds in the 80's: Burt behind the wheel of a fast car, like in his Bandit Movies. If you love NASCAR and Burt Reynolds, this movie is a must-see. If you only love one of this 2 things, I also recommend to watch it. If you like neither NASCAR nor Burt Reynolds, you still should give it a chance, but remember, this movie was far away from winning an Oscar Academy Award. It is the typical humor of the 80's. If you like movies like the Cannonball Movies, and Police Academy, you will also like that one.\n"
     ]
    }
   ],
   "source": [
    "j = random.randint(0,len(X_test)-7)\n",
    "for i in range(j,j+7):\n",
    "    print(y_pred[0])\n",
    "    ind = features_nd.tolist().index(X_test[i].tolist())\n",
    "    print(data[ind].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the overall accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8864\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(C) 2023-2024 by [Damir Cavar](http://damir.cavar.me/) <<dcavar@iu.edu>>**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
