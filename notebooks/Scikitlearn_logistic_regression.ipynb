{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn Logistic Regression\n",
    "\n",
    "(C) 2023 by [Damir Cavar](http://damir.cavar.me/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example problems are taken from the textbook Dan Jurafsky and James H. Martin (2023 draft) [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) chapter 5 on Logistic Regression. The code is written by [Damir Cavar](http://damir.cavar.me/) and simplified for use in the Advanced Natural Language Processing course taught at Indiana University in Fall 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "from secret import sigmoid\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import zipfile\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vader Lexicon file can be found in the NLTK data in `nltk-data/sentiment/vader_lexicon.zip`. It contains a list of tokens with sentiment ratings. Each line represents one token and the tab-seperated values are:\n",
    "- token\n",
    "- the mean of the human sentiment ratings\n",
    "- the Standard Deviation of the token\n",
    "- the list of 10 human ratings taken during experiments\n",
    "\n",
    "In the following the assumption is that the Vader lexicon is located in your `nltk-data`-folder. On Linux systems this is per default in your home directory. On Windows this is in your `AppData\\roaming` folder.\n",
    "\n",
    "We can read the Vader lexicon into a dictionary structure as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_data_folder = \"/home/damir/nltk_data\"\n",
    "vader_filename = \"vader_lexicon/vader_lexicon.txt\"\n",
    "vader_data = {}\n",
    "with zipfile.ZipFile(os.path.join(nltk_data_folder, \"sentiment\", 'vader_lexicon.zip')) as z:\n",
    "    if vader_filename in z.namelist():\n",
    "        with z.open(vader_filename) as f:\n",
    "            for l in f:\n",
    "                tokens = l.decode(encoding='utf-8').strip().split('\\t')\n",
    "                if len(tokens) != 4: continue\n",
    "                vader_data[tokens[0]] = (float(tokens[1]), float(tokens[2]), ast.literal_eval(tokens[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now request the scores for existing tokens from the `vadar_data` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.6, 0.66332, [2, 3, 3, 3, 4, 3, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(vader_data[\"admirable\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assume that positive scores indicate that the token is typical for positive sentiment, while negative scores represent negative sentiment. We can see that for example when pulling the scores for token `annoying`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1.7, 0.64031, [-1, -2, -1, -2, -1, -1, -2, -2, -3, -2])\n"
     ]
    }
   ],
   "source": [
    "print(vader_data[\"annoying\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the textbook the feature vector is generated using the following scores:\n",
    "- number of positive terms in text\n",
    "- number of negative terms\n",
    "- 1, if there is a *no* in the text, 0 if there is none\n",
    "- number of pronouns, all variants of 1st and 2nd person\n",
    "- 1 if there os a *!* in the text, 0 if there is none\n",
    "- the log of the number of tokens\n",
    "\n",
    "The following function generates a feature vector from some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_vector(text: str) -> list:\n",
    "    tokens = word_tokenize(text)\n",
    "    scores = [ vader_data.get(t, [0, 0]) for t in tokens ]\n",
    "    negative_terms = sum(1 for i in scores if i[0] < 0)\n",
    "    positive_terms = sum(1 for i in scores if i[0] > 0)\n",
    "    if \"no\" in tokens:\n",
    "        no_in_text = 1\n",
    "    else:\n",
    "        no_in_text = 0\n",
    "    pronouns = set( (\"I\", \"you\", \"me\", \"your\", \"mine\") )\n",
    "    count_pronouns = sum(1 for i in tokens if i in pronouns)\n",
    "    if \"!\" in tokens:\n",
    "        excl_in_text = 1\n",
    "    else:\n",
    "        excl_in_text = 0\n",
    "    return np.array([positive_terms, negative_terms, no_in_text, count_pronouns, excl_in_text, math.log(len(tokens))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use some sample text and generate a feature vector for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"It's hokey. There are virtually no surprises, and the writing is second-rate.\n",
    "So why was it so enjoyable? For one thing, the cast is great.\n",
    "Another nice touch is the music.\n",
    "I was overcome with the urge to get off the couch and start dancing.\n",
    "It sucked me in, and it'll do the same to you.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature vector is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.         2.         1.         3.         0.         4.21950771]\n"
     ]
    }
   ],
   "source": [
    "sample_text_vector = generate_feature_vector(sample_text)\n",
    "print(sample_text_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The textbook approach uses different vocabulary and entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text_vector_textbook = np.array([3, 2, 1, 3, 0, 4.19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([2.5, -5.0, -1.2, 0.5, 2.0, 0.7])\n",
    "b = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the sigmoid scores for the feature vectors generated from the Vader lexicon is given in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_positive = sigmoid( np.dot(weights, sample_text_vector) + b )\n",
    "sigmoid_negative = 1 - sigmoid_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is classified as `positive sentiment` with 96% likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9662243326599138 0.03377566734008619\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid_positive, sigmoid_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the textbook scores for the text vector, our sigmoid values are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_positive = sigmoid( np.dot(weights, sample_text_vector_textbook) + b )\n",
    "sigmoid_negative = 1 - sigmoid_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is still judged as `positive sentiment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6969888901292717 0.3030111098707283\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid_positive, sigmoid_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Weights\n",
    "\n",
    "The weights in the previos section have been manually set. In the following we will go over a strategy to learn those weights using the Cross-entropy Loss Function and Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the problem discussed in the textbook, the Cross-entropy Loss Function is defined as:\n",
    "\n",
    "$L_{CE}(\\hat{y},y) = -\\, log\\, p(y|x) = -\\, [y\\, log(\\hat{y}) + (1-y)\\, log(1-\\hat{y})] $\n",
    "\n",
    "If `y=1`, the second summand in the equation become `0`, thus we only look at:\n",
    "\n",
    "$L_{CE}(\\hat{y},1) = -\\, log\\, p(1|x) = -\\, 1\\, log(\\hat{y}) = -\\, log(\\hat{y})$\n",
    "\n",
    "For `y=0`, the first summand in the equation becomes `0`, this we only look at:\n",
    "\n",
    "$L_{CE}(\\hat{y},0) = -\\, log\\, p(0|x) = -\\, (1-0)\\, log(1-\\hat{y}) = -\\, log(1-\\hat{y}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y):\n",
    "  return -np.log(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y}$ is the `sigmoid` of the dot-product of the weight and feature vector after adding the bias value to it. That is, we can compute the `cross-entropy` from the `sigmoid` scores above\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid 0.6969888901292717 for y=1 Loss positive: 0.3609858079049309\n"
     ]
    }
   ],
   "source": [
    "cel_positive = cross_entropy_loss(sigmoid_positive)\n",
    "print(f\"sigmoid {sigmoid_positive} for y={1} Loss positive: {cel_positive}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid 0.3030111098707283 for y=0 Loss positive: 1.1939858079049306\n"
     ]
    }
   ],
   "source": [
    "cel_negative = cross_entropy_loss(sigmoid_negative)\n",
    "print(f\"sigmoid {sigmoid_negative} for y={0} Loss positive: {cel_negative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Loss function is paretrized by the weights $\\theta = (w,b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"\"\"It's hokey. There are virtually no surprises, and the writing is second-rate.\n",
    "So why was it so enjoyable? For one thing, the cast is great.\n",
    "Another nice touch is the music.\n",
    "I was overcome with the urge to get off the couch and start dancing.\n",
    "It sucked me in, and it'll do the same to you.\"\"\", 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stochastic gradient descent function in our simple classification task can be defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(data):\n",
    "    w = np.array([0, 0, 0, 0, 0, 0])\n",
    "    b = 0\n",
    "    learning_rate = 0.1\n",
    "    for text, y in data:\n",
    "        x = generate_feature_vector(text)\n",
    "        print(\"x:\", x)\n",
    "        y_hat = sigmoid( np.dot(w, x) + b )\n",
    "        print(\"y_hat:\", y_hat)\n",
    "        gradient_b = y_hat - y\n",
    "        print(\"gradient b:\", gradient_b)\n",
    "        b = b - learning_rate * gradient_b\n",
    "        print(\"new gradient b:\", b)\n",
    "        gradient_w = (y_hat - y) * x\n",
    "        print(\"gradient w:\", gradient_w)\n",
    "        w = gradient_w - learning_rate * gradient_w\n",
    "        print(\"new gradient w:\", w)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [4.         2.         1.         3.         0.         4.21950771]\n",
      "y_hat: 0.5\n",
      "gradient b: -0.5\n",
      "new gradient b: 0.05\n",
      "gradient w: [-2.         -1.         -0.5        -1.5        -0.         -2.10975385]\n",
      "new gradient w: [-1.8        -0.9        -0.45       -1.35        0.         -1.89877847]\n"
     ]
    }
   ],
   "source": [
    "w, b = stochastic_gradient_descent(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_silent(data):\n",
    "    w = np.array([0, 0, 0, 0, 0, 0])\n",
    "    b = 0\n",
    "    learning_rate = 0.1\n",
    "    for text, y in data:\n",
    "        x = generate_feature_vector(text)\n",
    "        y_hat = sigmoid( np.dot(w, x) + b )\n",
    "        gradient_b = y_hat - y\n",
    "        b = b - learning_rate * gradient_b\n",
    "        gradient_w = (y_hat - y) * x\n",
    "        w = gradient_w - learning_rate * gradient_w\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('.', 'data', 'reviews.csv'), newline='') as csvfile:\n",
    "    datareader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    header = next(datareader)\n",
    "    for row in datareader:\n",
    "        if len(row) == 2:\n",
    "            experiment_data.append( [row[0].strip(), int(row[1].strip())] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 50000\t Negative: 50000\n",
      "Total reviews: 100000\n"
     ]
    }
   ],
   "source": [
    "count_positive = sum([ 1 for x in experiment_data if x[1] == 1 ])\n",
    "count_negative = sum([ 1 for x in experiment_data if x[1] == 0 ])\n",
    "print(f\"Positive: {count_positive}\\t Negative: {count_negative}\")\n",
    "print(\"Total reviews:\", len(experiment_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\", 0], [\"This is an example of why the majority of action films are the same. Generic and boring, there's really nothing worth watching here. A complete waste of the then barely-tapped talents of Ice-T and Ice Cube, who've each proven many times over that they are capable of acting, and acting well. Don't bother with this one, go see New Jack City, Ricochet or watch New York Undercover for Ice-T, or Boyz n the Hood, Higher Learning or Friday for Ice Cube and see the real deal. Ice-T's horribly cliched dialogue alone makes this film grate at the teeth, and I'm still wondering what the heck Bill Paxton was doing in this film? And why the heck does he always play the exact same character? From Aliens onward, every film I've seen with Bill Paxton has him playing the exact same irritating character, and at least in Aliens his character died, which made it somewhat gratifying... Overall, this is second-rate action trash. There are countless better films to see, and if you really want to see this one, watch Judgement Night, which is practically a carbon copy but has better acting and a better script. The only thing that made this at all worth watching was a decent hand on the camera - the cinematography was almost refreshing, which comes close to making up for the horrible film itself - but not quite. 4/10.\", 0], [\"First of all I hate those moronic rappers, who could'nt act if they had a gun pressed against their foreheads. All they do is curse and shoot each other and acting like clichÃ©'e version of gangsters. The movie doesn't take more than five minutes to explain what is going on before we're already at the warehouse There is not a single sympathetic character in this movie, except for the homeless guy, who is also the only one with half a brain. Bill Paxton and William Sadler are both hill billies and Sadlers character is just as much a villain as the gangsters. I did'nt like him right from the start. The movie is filled with pointless violence and Walter Hills specialty: people falling through windows with glass flying everywhere. There is pretty much no plot and it is a big problem when you root for no-one. Everybody dies, except from Paxton and the homeless guy and everybody get what they deserve. The only two black people that can act is the homeless guy and the junkie but they're actors by profession, not annoying ugly brain dead rappers. Stay away from this crap and watch 48 hours 1 and 2 instead. At lest they have characters you care about, a sense of humor and nothing but real actors in the cast.\", 0]]\n",
      "[-1.8        -0.9        -0.45       -1.35        0.         -1.89877847] 0.05\n"
     ]
    }
   ],
   "source": [
    "print(experiment_data[:3])\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = stochastic_gradient_descent_silent(experiment_data[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.4        5.4        0.         2.7        0.         4.59535093] -4.94999999999999\n"
     ]
    }
   ],
   "source": [
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data, w, b):\n",
    "    res = []\n",
    "    for text, y in data:\n",
    "        x = generate_feature_vector(text)\n",
    "        y_hat = sigmoid( np.dot(w, x) + b )\n",
    "        if y_hat > .5:\n",
    "            y_hat = 1\n",
    "        else: y_hat = 0\n",
    "        res.append( (y, y_hat) )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(experiment_data[-20:], w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(1, 1): 20})\n"
     ]
    }
   ],
   "source": [
    "print(counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
