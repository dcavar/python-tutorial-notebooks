{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Feed Forward Neural Network from Scratch - Sentiment Analysis\n",
    "\n",
    "© 2025 by [Damir Cavar](http://damir.cavar.me/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites:**\n",
    "\n",
    "We will need to install the following modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U numpy\n",
    "!pip install -U nltk\n",
    "!pip install -U scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the following modules and some specfic functions from some of these modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Example\n",
    "\n",
    "In the following we will use the reviews data that we used in the Logistic Regression section in notebook *Scikitlearn_logistic_regression_2*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Reviews corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_data = []\n",
    "with open(os.path.join('.', 'data', 'reviews.csv'), newline='') as csvfile:\n",
    "    datareader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    header = next(datareader)\n",
    "    for row in datareader:\n",
    "        if len(row) == 2:\n",
    "            experiment_data.append( [row[0].strip(), int(row[1].strip())] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\", 0], [\"This is an example of why the majority of action films are the same. Generic and boring, there's really nothing worth watching here. A complete waste of the then barely-tapped talents of Ice-T and Ice Cube, who've each proven many times over that they are capable of acting, and acting well. Don't bother with this one, go see New Jack City, Ricochet or watch New York Undercover for Ice-T, or Boyz n the Hood, Higher Learning or Friday for Ice Cube and see the real deal. Ice-T's horribly cliched dialogue alone makes this film grate at the teeth, and I'm still wondering what the heck Bill Paxton was doing in this film? And why the heck does he always play the exact same character? From Aliens onward, every film I've seen with Bill Paxton has him playing the exact same irritating character, and at least in Aliens his character died, which made it somewhat gratifying... Overall, this is second-rate action trash. There are countless better films to see, and if you really want to see this one, watch Judgement Night, which is practically a carbon copy but has better acting and a better script. The only thing that made this at all worth watching was a decent hand on the camera - the cinematography was almost refreshing, which comes close to making up for the horrible film itself - but not quite. 4/10.\", 0]]\n",
      "Text:\n",
      " Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\n",
      "Value:\n",
      " 0\n",
      "Number of records: 50000\n"
     ]
    }
   ],
   "source": [
    "print(experiment_data[:2])\n",
    "print(\"Text:\\n\", experiment_data[0][0])\n",
    "print(\"Value:\\n\", experiment_data[0][1])\n",
    "print(\"Number of records:\", len(experiment_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Vader lexicon for lexical sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dictionary = {}\n",
    "with open(os.path.join('.', 'data', 'vader_lexicon.txt'), mode='r', encoding='utf-8') as ifile:\n",
    "    lines = ifile.readlines()\n",
    "    sentiment_dictionary = { y[0]: y[1] for y in [ x.split('\\t') for x in lines ] if len(y) == 4 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_dictionary[\"adventurer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the set of 1st and 2nd person pronouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns = {\"i\", \"me\", \"my\", \"mine\", \"you\", \"yours\", \"yourself\", \"myself\", \"we\", \"us\", \"our\", \"ours\", \"ourselves\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorization function for the review texts is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(text: str) -> list:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    scores = [ float(sentiment_dictionary.get(t, 0.0)) for t in tokens ]\n",
    "    positive = len([ s for s in scores if s > 0 ])\n",
    "    negative = len([ s for s in scores if s < 0 ])\n",
    "    if \"no\" in tokens:\n",
    "        no_present = 1\n",
    "    else:\n",
    "        no_present = 0\n",
    "    counts = Counter(tokens)\n",
    "    pronoun_count = 0\n",
    "    for x in set(counts.keys()).intersection(pronouns):\n",
    "        pronoun_count += counts[x]\n",
    "    if \"!\" in tokens:\n",
    "        exclamation = 1\n",
    "    else:\n",
    "        exclamation = 0\n",
    "    return [positive, negative, no_present, pronoun_count, exclamation, math.log(len(tokens))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the vectors for all the corpus texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [ np.array(vectorizer(e[0])) for e in experiment_data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 6\n",
      "[13.          8.          0.          3.          0.          5.58349631] \n",
      " [\"This is an example of why the majority of action films are the same. Generic and boring, there's really nothing worth watching here. A complete waste of the then barely-tapped talents of Ice-T and Ice Cube, who've each proven many times over that they are capable of acting, and acting well. Don't bother with this one, go see New Jack City, Ricochet or watch New York Undercover for Ice-T, or Boyz n the Hood, Higher Learning or Friday for Ice Cube and see the real deal. Ice-T's horribly cliched dialogue alone makes this film grate at the teeth, and I'm still wondering what the heck Bill Paxton was doing in this film? And why the heck does he always play the exact same character? From Aliens onward, every film I've seen with Bill Paxton has him playing the exact same irritating character, and at least in Aliens his character died, which made it somewhat gratifying... Overall, this is second-rate action trash. There are countless better films to see, and if you really want to see this one, watch Judgement Night, which is practically a carbon copy but has better acting and a better script. The only thing that made this at all worth watching was a decent hand on the camera - the cinematography was almost refreshing, which comes close to making up for the horrible film itself - but not quite. 4/10.\", 0] \n",
      " This is an example of why the majority of action films are the same. Generic and boring, there's really nothing worth watching here. A complete waste of the then barely-tapped talents of Ice-T and Ice Cube, who've each proven many times over that they are capable of acting, and acting well. Don't bother with this one, go see New Jack City, Ricochet or watch New York Undercover for Ice-T, or Boyz n the Hood, Higher Learning or Friday for Ice Cube and see the real deal. Ice-T's horribly cliched dialogue alone makes this film grate at the teeth, and I'm still wondering what the heck Bill Paxton was doing in this film? And why the heck does he always play the exact same character? From Aliens onward, every film I've seen with Bill Paxton has him playing the exact same irritating character, and at least in Aliens his character died, which made it somewhat gratifying... Overall, this is second-rate action trash. There are countless better films to see, and if you really want to see this one, watch Judgement Night, which is practically a carbon copy but has better acting and a better script. The only thing that made this at all worth watching was a decent hand on the camera - the cinematography was almost refreshing, which comes close to making up for the horrible film itself - but not quite. 4/10. \n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "print(len(X), len(X[0]))\n",
    "print(X[1], \"\\n\", experiment_data[1], \"\\n\", experiment_data[1][0], \"\\n\", experiment_data[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the truth values for the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([ float(e[1]) for e in experiment_data ]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.          9.          1.          8.          0.          5.23110862]\n",
      " [13.          8.          0.          3.          0.          5.58349631]\n",
      " [ 6.         14.          1.          5.          0.          5.49306144]\n",
      " ...\n",
      " [22.         18.          0.         12.          0.          6.14632926]\n",
      " [11.          0.          0.         12.          0.          5.32300998]\n",
      " [11.          7.          1.          1.          1.          5.27299956]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the following functions for inferencing and backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "\t\"\"\"Sigmoid activation function\"\"\"\n",
    "\treturn 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "\t\"\"\"Derivative of sigmoid function\"\"\"\n",
    "\ts = sigmoid(z)\n",
    "\treturn s * (1 - s)\n",
    "\n",
    "def relu(z):\n",
    "\t\"\"\"ReLU activation function\"\"\"\n",
    "\treturn np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "\t\"\"\"Derivative of ReLU function\"\"\"\n",
    "\treturn (z > 0.0).astype(float)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "\t\"\"\"\n",
    "\tInput:\n",
    "\t\ty_true: True labels   \n",
    "\t\ty_pred: Predicted probabilities\n",
    "\tOutput:\n",
    "\t\tloss: Cross-entropy loss\n",
    "\t\"\"\"\n",
    "\tm = y_true.shape[0]\n",
    "\tepsilon = 1e-15  # for numerical stability - no log(0)\n",
    "\ty_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "\tloss = -(1/m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\treturn loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the variables and weights for the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.         9.         1.         8.         0.         5.23110862]\n",
      "W:\n",
      " [[0.99217103 0.07293832 0.46161288 0.9837251  0.13113083 0.51543797]\n",
      " [0.8168424  0.46521118 0.94027466 0.89042465 0.35039789 0.45721929]\n",
      " [0.53580728 0.81325612 0.55401231 0.55926888 0.98531595 0.79650713]]\n",
      "b:\n",
      " [0.80022043 0.11151789 0.44289653]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "num_features = len(X[0])\n",
    "num_rows = 3\n",
    "W = np.random.rand(num_rows, num_features)\n",
    "print(\"W:\\n\", W)\n",
    "b = np.random.rand(num_rows)\n",
    "print(\"b:\\n\", b)\n",
    "#print(\"W+b:\\n\", relu(W @ x[0] + b) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer receives a *num-rows*-dimentional vector and generates one output z-score. Initialize the output layer of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U:\n",
      " [[0.51257843 0.57473851 0.04378136]]\n",
      "bu:\n",
      " [0.39124699]\n",
      "[21.41393034 22.10543579 21.77924589]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "U = np.random.rand(1, num_rows)\n",
    "print(\"U:\\n\", U)\n",
    "bu = np.random.rand(1)\n",
    "print(\"bu:\\n\", bu)\n",
    "test = W @ X[0] + b\n",
    "print(test)\n",
    "print(sigmoid(U @ test + bu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data set in the following code is the entire data set. As an exercise use some function to randomly select a portion for training and another portion for testing. Use only the true randomly selected training data set for the following code. This code will run a complete inferencing cycle through the training data set, which is in this case the entire data set. Change the code and set up a real evaluation of training and testing, and computing of the F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for input_vector, truth in zip(X, y):\n",
    "    z = (W @ input_vector) + b\n",
    "    a = relu(z)\n",
    "    c = sigmoid((U @ a) + bu)\n",
    "    results.append( (truth, c) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of results: 50000\n"
     ]
    }
   ],
   "source": [
    "print(\"# of results:\", len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 25000\n",
      "True negatives: 0\n",
      "False positives: 25000\n",
      "False negatives: 0\n"
     ]
    }
   ],
   "source": [
    "counting_true_negatives = 0\n",
    "counting_true_positives = 0\n",
    "counting_false_positives = 0\n",
    "counting_false_negatives = 0\n",
    "for res in results:\n",
    "    # print(res[0], res[1][0])\n",
    "    if res[0] == 1:\n",
    "        if res[1][0] >= 0.5:\n",
    "            counting_true_positives += 1\n",
    "        else:\n",
    "            counting_false_negatives += 1\n",
    "    else:\n",
    "        if res[1][0] < 0.5:\n",
    "            counting_true_negatives += 1\n",
    "        else:\n",
    "            counting_false_positives += 1\n",
    "print(\"True positives:\", counting_true_positives)\n",
    "print(\"True negatives:\", counting_true_negatives)\n",
    "print(\"False positives:\", counting_false_positives)\n",
    "print(\"False negatives:\", counting_false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following FFNN class uses:\n",
    "\n",
    "**Xavier/Glorot initialization**\n",
    "\n",
    "This makes sure that the weights are samples from a normal or uniform distribution with the standard deviation scaled based on the number of input and output neurons in the layer. This can prevent vanishing or exploding gradients by ensuring the variance of activations and gradients to remain constant across all layers, which allows for more stable and efficient training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    \"\"\"A simple feedforward neural network with one hidden layer.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=6, hidden_dim=8, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize a FFNN:\n",
    "        \"\"\"\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # Initialize weights with Xavier/Glorot initialization\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_dim, 1) * np.sqrt(2.0 / hidden_dim)\n",
    "        self.b2 = np.zeros((1, 1))\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        Forward ppass / inferencing\n",
    "        Input: X: = input data, the shape is defined by the batch_size and the input_dim (= feature vector length)\n",
    "        Return: Predictions as a single vector, the shape is batch_size and 1 (= one scalar per input sample)\n",
    "        \"\"\"\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward_pass(self, X, y, output):\n",
    "        \"\"\"\n",
    "        Backward pass\n",
    "        Inputs:\n",
    "            X = input vectors, the shape is the batch_size times the input_dim (feature vector length)\n",
    "            y = True labels of shape (batch_size, 1)\n",
    "            output = Predicted values of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        m = X.shape[0]  # get the batch size\n",
    "        # Compute gradients for the output layer:\n",
    "        # - using binary cross-entropy loss with sigmoid output\n",
    "        dz2 = output - y  # derivative of loss w.r.t z2\n",
    "        dW2 = (1/m) * np.dot(self.a1.T, dz2)\n",
    "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "        # Compute gradients for hidden layer\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        dz1 = da1 * relu_derivative(self.z1)\n",
    "        dW1 = (1/m) * np.dot(X.T, dz1)\n",
    "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "        # Store gradients\n",
    "        self.dW1 = dW1\n",
    "        self.db1 = db1\n",
    "        self.dW2 = dW2\n",
    "        self.db2 = db2\n",
    "\n",
    "    def update_parameters(self):\n",
    "        \"\"\"Update parameters using gradient descent.\"\"\"\n",
    "        self.W1 -= self.lr * self.dW1\n",
    "        self.b1 -= self.lr * self.db1\n",
    "        self.W2 -= self.lr * self.dW2\n",
    "        self.b2 -= self.lr * self.db2\n",
    "\n",
    "    def train(self, X, y, epochs=1000, batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        Training loop\n",
    "        Inputs:\n",
    "            X: Training data, the shape is n_samples and input_dim (= feature vector length)\n",
    "            y: Training labels, the shape is n_samples and 1 (= one scalar per input sample)\n",
    "            epochs: Number of training epochs, default 1,000\n",
    "            batch_size: Size of mini-batches, default 32\n",
    "            verbose: Whether to print loss, default True\n",
    "        Outputs:\n",
    "            losses: List of loss values per epoch\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            epoch_loss = 0\n",
    "            n_batches = 0\n",
    "\n",
    "            # Mini-batch gradient descent\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                output = self.forward_pass(X_batch)\n",
    "\n",
    "                # Compute loss\n",
    "                batch_loss = cross_entropy_loss(y_batch, output)\n",
    "                epoch_loss += batch_loss\n",
    "                n_batches += 1\n",
    "\n",
    "                # Backward pass with parameter updates after each batch\n",
    "                self.backward_pass(X_batch, y_batch, output)\n",
    "                self.update_parameters()\n",
    "\n",
    "            # Average loss for epoch\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "            losses.append(avg_loss)\n",
    "\n",
    "            if verbose and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        return self.forward_pass(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a brute force method to split the data set into a training and test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The better method is to use a random selection of the training and test set. Each time we call the *scikit learn* function *train_test_split* we will get a new randomly selected data set. We can repeat the experiment using a 10-fold rotation to average the error and measure the F1-score, by 10 times randomly selecting new training and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a model using our FFNN class above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFNN(input_dim=(len(X[0])), hidden_dim=8, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the network parameters using the training data using 500 epochs and a batch size of 32. This means, the network will see each training example 500 times. Every sample of the data is processed by the network once in an epoch. The processing is iterative by batches. A batch could be the entire data set or subsets of the data in smaller batches. The training method returns the losses by epoch in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000, Loss: 0.652037\n",
      "Epoch 100/1000, Loss: 0.549889\n",
      "Epoch 200/1000, Loss: 0.549259\n",
      "Epoch 300/1000, Loss: 0.548848\n",
      "Epoch 400/1000, Loss: 0.548508\n",
      "Epoch 500/1000, Loss: 0.548304\n",
      "Epoch 600/1000, Loss: 0.548336\n",
      "Epoch 700/1000, Loss: 0.548067\n",
      "Epoch 800/1000, Loss: 0.547589\n",
      "Epoch 900/1000, Loss: 0.547541\n",
      "Epoch 999/1000, Loss: 0.547559\n"
     ]
    }
   ],
   "source": [
    "losses = model.train(X_train, y_train, epochs=1000, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of epochs is a hyperparameter that can impact the model. With too few epochs the result could be an underfitted model that has not learned enough of the data properties. With too many epochs the result could be an overfitted model that performs poorly on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model using our selected test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test loss as tje Cross-entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = cross_entropy_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary = (y_pred > 0.5).astype(float)\n",
    "accuracy = np.mean(y_pred_binary == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.552522\n",
      "Test Accuracy: 0.7195\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Loss: {test_loss:.6f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© 2025 by [Damir Cavar](http://damir.cavar.me/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
