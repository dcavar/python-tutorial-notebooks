{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Feed Forward Neural Network from Scratch - Sentiment Analysis\n",
    "\n",
    "Â© 2025 by [Damir Cavar](http://damir.cavar.me/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites:**\n",
    "\n",
    "We will need to install the following modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U numpy\n",
    "!pip install -U nltk\n",
    "!pip install -U scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the following modules and some specfic functions from some of these modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Example\n",
    "\n",
    "In the following we will use the reviews data that we used in the Logistic Regression section in notebook *Scikitlearn_logistic_regression_2*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Reviews corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_data = []\n",
    "with open(os.path.join('.', 'data', 'reviews.csv'), newline='') as csvfile:\n",
    "    datareader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    header = next(datareader)\n",
    "    for row in datareader:\n",
    "        if len(row) == 2:\n",
    "            experiment_data.append( [row[0].strip(), int(row[1].strip())] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiment_data[:2])\n",
    "print(\"Text:\\n\", experiment_data[0][0])\n",
    "print(\"Value:\\n\", experiment_data[0][1])\n",
    "print(\"Number of records:\", len(experiment_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Vader lexicon for lexical sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dictionary = {}\n",
    "with open(os.path.join('.', 'data', 'vader_lexicon.txt'), mode='r', encoding='utf-8') as ifile:\n",
    "    lines = ifile.readlines()\n",
    "    sentiment_dictionary = { y[0]: y[1] for y in [ x.split('\\t') for x in lines ] if len(y) == 4 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentiment_dictionary[\"adventurer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the set of 1st and 2nd person pronouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns = {\"i\", \"me\", \"my\", \"mine\", \"you\", \"yours\", \"yourself\", \"myself\", \"we\", \"us\", \"our\", \"ours\", \"ourselves\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorization function for the review texts is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(text: str) -> list:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    scores = [ float(sentiment_dictionary.get(t, 0.0)) for t in tokens ]\n",
    "    positive = len([ s for s in scores if s > 0 ])\n",
    "    negative = len([ s for s in scores if s < 0 ])\n",
    "    if \"no\" in tokens:\n",
    "        no_present = 1\n",
    "    else:\n",
    "        no_present = 0\n",
    "    counts = Counter(tokens)\n",
    "    pronoun_count = 0\n",
    "    for x in set(counts.keys()).intersection(pronouns):\n",
    "        pronoun_count += counts[x]\n",
    "    if \"!\" in tokens:\n",
    "        exclamation = 1\n",
    "    else:\n",
    "        exclamation = 0\n",
    "    return [positive, negative, no_present, pronoun_count, exclamation, math.log(len(tokens))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the vectors for all the corpus texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [ np.array(vectorizer(e[0])) for e in experiment_data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X), len(X[0]))\n",
    "print(X[1], \"\\n\", experiment_data[1], \"\\n\", experiment_data[1][0], \"\\n\", experiment_data[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the truth values for the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([ float(e[1]) for e in experiment_data ]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(x)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the following functions for inferencing and backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "\t\"\"\"Sigmoid activation function\"\"\"\n",
    "\treturn 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "\t\"\"\"Derivative of sigmoid function\"\"\"\n",
    "\ts = sigmoid(z)\n",
    "\treturn s * (1 - s)\n",
    "\n",
    "def relu(z):\n",
    "\t\"\"\"ReLU activation function\"\"\"\n",
    "\treturn np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "\t\"\"\"Derivative of ReLU function\"\"\"\n",
    "\treturn (z > 0.0).astype(float)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "\t\"\"\"\n",
    "\tInput:\n",
    "\t\ty_true: True labels   \n",
    "\t\ty_pred: Predicted probabilities\n",
    "\tOutput:\n",
    "\t\tloss: Cross-entropy loss\n",
    "\t\"\"\"\n",
    "\tm = y_true.shape[0]\n",
    "\tepsilon = 1e-15  # for numerical stability - no log(0)\n",
    "\ty_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "\tloss = -(1/m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\treturn loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the variables and weights for the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])\n",
    "num_features = len(X[0])\n",
    "num_rows = 3\n",
    "W = np.random.rand(num_rows, num_features)\n",
    "print(\"W:\\n\", W)\n",
    "b = np.random.rand(num_rows)\n",
    "print(\"b:\\n\", b)\n",
    "print(\"W+b:\\n\", relu(W @ x[0] + b) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer receives a *num-rows*-dimentional vector and generates one output z-score. Initialize the output layer of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.random.rand(1, num_rows)\n",
    "print(\"U:\\n\", U)\n",
    "bu = np.random.rand(1)\n",
    "print(\"bu:\\n\", bu)\n",
    "test = W @ X[0] + b\n",
    "print(test)\n",
    "print(sigmoid(U @ test + bu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data set in the following code is the entire data set. As an exercise use some function to randomly select a portion for training and another portion for testing. Use only the true randomly selected training data set for the following code. This code will run a complete inferencing cycle through the training data set, which is in this case the entire data set. Change the code and set up a real evaluation of training and testing, and computing of the F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for input_vector, truth in zip(X, y):\n",
    "    z = (W @ input_vector) + b\n",
    "    a = relu(z)\n",
    "    c = sigmoid((U @ a) + bu)\n",
    "    results.append( (truth, c) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# of results:\", len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_true_negatives = 0\n",
    "counting_true_positives = 0\n",
    "counting_false_positives = 0\n",
    "counting_false_negatives = 0\n",
    "for res in results:\n",
    "    # print(res[0], res[1][0])\n",
    "    if res[0] == 1:\n",
    "        if res[1][0] >= 0.5:\n",
    "            counting_true_positives += 1\n",
    "        else:\n",
    "            counting_false_negatives += 1\n",
    "    else:\n",
    "        if res[1][0] < 0.5:\n",
    "            counting_true_negatives += 1\n",
    "        else:\n",
    "            counting_false_positives += 1\n",
    "print(\"True positives:\", counting_true_positives)\n",
    "print(\"True negatives:\", counting_true_negatives)\n",
    "print(\"False positives:\", counting_false_positives)\n",
    "print(\"False negatives:\", counting_false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following FFNN class uses:\n",
    "\n",
    "**Xavier/Glorot initialization**\n",
    "\n",
    "This makes sure that the weights are samples from a normal or uniform distribution with the standard deviation scaled based on the number of input and output neurons in the layer. This can prevent vanishing or exploding gradients by ensuring the variance of activations and gradients to remain constant across all layers, which allows for more stable and efficient training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    \"\"\"A simple feedforward neural network with one hidden layer.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=6, hidden_dim=8, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize a FFNN:\n",
    "        \"\"\"\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # Initialize weights with Xavier/Glorot initialization\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_dim, 1) * np.sqrt(2.0 / hidden_dim)\n",
    "        self.b2 = np.zeros((1, 1))\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        Forward ppass / inferencing\n",
    "        Input: X: = input data, the shape is defined by the batch_size and the input_dim (= feature vector length)\n",
    "        Return: Predictions as a single vector, the shape is batch_size and 1 (= one scalar per input sample)\n",
    "        \"\"\"\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward_pass(self, X, y, output):\n",
    "        \"\"\"\n",
    "        Backward pass\n",
    "        Inputs:\n",
    "            X = input vectors, the shape is the batch_size times the input_dim (feature vector length)\n",
    "            y = True labels of shape (batch_size, 1)\n",
    "            output = Predicted values of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        m = X.shape[0]  # get the batch size\n",
    "        # Compute gradients for the output layer:\n",
    "        # - using binary cross-entropy loss with sigmoid output\n",
    "        dz2 = output - y  # derivative of loss w.r.t z2\n",
    "        dW2 = (1/m) * np.dot(self.a1.T, dz2)\n",
    "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "        # Compute gradients for hidden layer\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        dz1 = da1 * relu_derivative(self.z1)\n",
    "        dW1 = (1/m) * np.dot(X.T, dz1)\n",
    "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "        # Store gradients\n",
    "        self.dW1 = dW1\n",
    "        self.db1 = db1\n",
    "        self.dW2 = dW2\n",
    "        self.db2 = db2\n",
    "\n",
    "    def update_parameters(self):\n",
    "        \"\"\"Update parameters using gradient descent.\"\"\"\n",
    "        self.W1 -= self.lr * self.dW1\n",
    "        self.b1 -= self.lr * self.db1\n",
    "        self.W2 -= self.lr * self.dW2\n",
    "        self.b2 -= self.lr * self.db2\n",
    "\n",
    "    def train(self, X, y, epochs=1000, batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        Training loop\n",
    "        Inputs:\n",
    "            X: Training data, the shape is n_samples and input_dim (= feature vector length)\n",
    "            y: Training labels, the shape is n_samples and 1 (= one scalar per input sample)\n",
    "            epochs: Number of training epochs, default 1,000\n",
    "            batch_size: Size of mini-batches, default 32\n",
    "            verbose: Whether to print loss, default True\n",
    "        Outputs:\n",
    "            losses: List of loss values per epoch\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            epoch_loss = 0\n",
    "            n_batches = 0\n",
    "\n",
    "            # Mini-batch gradient descent\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                output = self.forward_pass(X_batch)\n",
    "\n",
    "                # Compute loss\n",
    "                batch_loss = cross_entropy_loss(y_batch, output)\n",
    "                epoch_loss += batch_loss\n",
    "                n_batches += 1\n",
    "\n",
    "                # Backward pass with parameter updates after each batch\n",
    "                self.backward_pass(X_batch, y_batch, output)\n",
    "                self.update_parameters()\n",
    "\n",
    "            # Average loss for epoch\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "            losses.append(avg_loss)\n",
    "\n",
    "            if verbose and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        return self.forward_pass(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a brute force method to split the data set into a training and test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The better method is to use a random selection of the training and test set. Each time we call the *scikit learn* function *train_test_split* we will get a new randomly selected data set. We can repeat the experiment using a 10-fold rotation to average the error and measure the F1-score, by 10 times randomly selecting new training and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a model using our FFNN class above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFNN(input_dim=(len(X[0])), hidden_dim=8, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the network parameters using the training data using 500 epochs and a batch size of 32. This means, the network will see each training example 500 times. Every sample of the data is processed by the network once in an epoch. The processing is iterative by batches. A batch could be the entire data set or subsets of the data in smaller batches. The training method returns the losses by epoch in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = model.train(X_train, y_train, epochs=500, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of epochs is a hyperparameter that can impact the model. With too few epochs the result could be an underfitted model that has not learned enough of the data properties. With too many epochs the result could be an overfitted model that performs poorly on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model using our selected test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test loss as tje Cross-entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = cross_entropy_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary = (y_pred > 0.5).astype(float)\n",
    "accuracy = np.mean(y_pred_binary == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Loss: {test_loss:.6f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â© 2025 by [Damir Cavar](http://damir.cavar.me/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
